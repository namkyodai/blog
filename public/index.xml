<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description></description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 19 Oct 2019 11:08:22 +0800</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title></title>
      <link>/</link>
    </image>
    
    <item>
      <title>Lamesa 2 - Maynilad</title>
      <link>/project/2016-lamesa2/</link>
      <pubDate>Sat, 19 Oct 2019 11:08:22 +0800</pubDate>
      <guid>/project/2016-lamesa2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Plant Audit - Maynilad</title>
      <link>/project/2018-plantaudit/</link>
      <pubDate>Sat, 19 Oct 2019 11:08:22 +0800</pubDate>
      <guid>/project/2018-plantaudit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fuel Efficiency - Regression Analysis</title>
      <link>/post/2019-10-10-fuel-efficiency/</link>
      <pubDate>Thu, 10 Oct 2019 23:30:45 +0800</pubDate>
      <guid>/post/2019-10-10-fuel-efficiency/</guid>
      <description>

&lt;p&gt;This example shows a very simple regression analysis using fuel data of 38 cars. Code is copied from the book “Data Mining and Business Analytics with R” with minor modification on the graphs.&lt;/p&gt;

&lt;p&gt;There are 2 learning points to be remembered with this example.&lt;/p&gt;

&lt;h1 id=&#34;leaps-package-with-regsubsets-fuction&#34;&gt;Leaps package with regsubsets fuction.&lt;/h1&gt;

&lt;p&gt;Regsubsets function allows to perform regression on subsets of data, particularly useful to compare across model’s predictors to understand the fit of the model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.rdocumentation.org/packages/leaps/versions/2.1-1/topics/regsubsets&#34; target=&#34;_blank&#34;&gt;https://www.rdocumentation.org/packages/leaps/versions/2.1-1/topics/regsubsets&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;cross-validation&#34;&gt;Cross-Validation&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Cross-validation removes one case from the data set of n cases, fits the model to the reduced data set, and predicts the response of that one case that has been removed from the estimation. This is repeated for each of the n cases. The summary statistics of the n genuine out-of-sample prediction errors (mean error, root mean square error, mean absolute percent error) help us assess the out-of-sample prediction performance. Cross-validation is very informative as it evaluates the model on new data. We find that the model with all six regressors performs better. It leads to a mean absolute percent error of about 6.75% (as compared to 8.23% for the model with weight as the only regressor).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; FuelEff
    GPM    WT DIS NC  HP  ACC ET
1  5.917 4.360 350  8 155 14.9  1
2  6.452 4.054 351  8 142 14.3  1
3  5.208 3.605 267  8 125 15.0  1
4  5.405 3.940 360  8 150 13.0  1
5  3.333 2.155  98  4  68 16.5  0
6  3.636 2.560 134  4  95 14.2  0
7  3.676 2.300 119  4  97 14.7  0
8  3.236 2.230 105  4  75 14.5  0
9  4.926 2.830 131  5 103 15.9  0
10 5.882 3.140 163  6 125 13.6  0
11 4.630 2.795 121  4 115 15.7  0
12 6.173 3.410 163  6 133 15.8  0
13 4.854 3.380 231  6 105 15.8  0
14 4.808 3.070 200  6  85 16.7  0
15 5.376 3.620 225  6 110 18.7  0
16 5.525 3.410 258  6 120 15.1  0
17 5.882 3.840 305  8 130 15.4  1
18 5.682 3.725 302  8 129 13.4  1
19 6.061 3.955 351  8 138 13.2  1
20 5.495 3.830 318  8 135 15.2  1
21 3.774 2.585 140  4  88 14.4  0
22 4.566 2.910 171  6 109 16.6  1
23 2.933 1.975  86  4  65 15.2  0
24 2.849 1.915  98  4  80 14.4  0
25 3.650 2.670 121  4  80 15.0  0
26 3.175 1.990  89  4  71 14.9  0
27 3.390 2.135  98  4  68 16.6  0
28 3.521 2.670 151  4  90 16.0  0
29 3.472 2.595 173  6 115 11.3  1
30 3.731 2.700 173  6 115 12.9  1
31 2.985 2.556 151  4  90 13.2  0
32 2.924 2.200 105  4  70 13.2  0
33 3.145 2.020  85  4  65 19.2  0
34 2.681 2.130  91  4  69 14.7  0
35 3.279 2.190  97  4  78 14.1  0
36 4.545 2.815 146  6  97 14.5  0
37 4.651 2.600 121  4 110 12.8  0
38 3.135 1.925  89  4  71 14.0  0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;code&#34;&gt;CODE&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## first we read in the data
FuelEff &amp;lt;- read.csv(&amp;quot;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/FuelEfficiency.csv&amp;quot;)
#FuelEff &amp;lt;- read.csv(&amp;quot;FuelEfficiency.csv&amp;quot;)

FuelEff

#----------------------------
plot.new()
#par(mfrow=c(1,1))
par(mar=c(4,4,1,1)+0.1,mfrow=c(3,3),bg=&amp;quot;white&amp;quot;,cex = 1, cex.main = 1)
plot(GPM~MPG,data=FuelEff)
plot(GPM~WT,data=FuelEff)
plot(GPM~DIS,data=FuelEff)
plot(GPM~NC,data=FuelEff)
plot(GPM~HP,data=FuelEff)
plot(GPM~ACC,data=FuelEff)
plot(GPM~ET,data=FuelEff)

dev.copy(png,&#39;fueleff_xyplot.png&#39;,width = 800, height = 800)
dev.off()


FuelEff=FuelEff[-1] #ignore the MPG column
FuelEff

## regression on all data
m1=lm(GPM~.,data=FuelEff)
summary(m1)

cor(FuelEff)

## best subset regression in R
library(leaps)
X=FuelEff[,2:7]
y=FuelEff[,1]

#use the regsubsets function from package leaps to compute regression of the subsets
#https://www.rdocumentation.org/packages/leaps/versions/2.1-1/topics/regsubsets


out=summary(regsubsets(X,y,nbest=2,nvmax=ncol(X)))
tab=cbind(out$which,out$rsq,out$adjr2,out$cp)
tab

m2=lm(GPM~WT,data=FuelEff)
summary(m2)

## cross-validation (leave one out) for the model on all six regressors
n=length(FuelEff$GPM)
diff=dim(n)
percdiff=dim(n)
for (k in 1:n) {
  train1=c(1:n)
  train=train1[train1!=k]
  ## the R expression &amp;quot;train1[train1!=k]&amp;quot; picks from train1 those
  ## elements that are different from k and stores those elements in the
  ## object train.
  ## For k=1, train consists of elements that are different from 1; that
  ## is 2, 3, …, n.
  m1=lm(GPM~.,data=FuelEff[train,])
  pred=predict(m1,newdat=FuelEff[-train,]) #adding the new data, which is ignored earlier
  obs=FuelEff$GPM[-train]
  diff[k]=obs-pred
  percdiff[k]=abs(diff[k])/obs
}
me=mean(diff)
rmse=sqrt(mean(diff**2))
mape=100*(mean(percdiff))
me   # mean error
rmse # root mean square error
mape # mean absolute percent error

## cross-validation (leave one out) for the model on weight only
n=length(FuelEff$GPM)
diff=dim(n)
percdiff=dim(n)
for (k in 1:n) {
  train1=c(1:n)
  train=train1[train1!=k]
  m2=lm(GPM~WT,data=FuelEff[train,])
  pred=predict(m2,newdat=FuelEff[-train,])
  obs=FuelEff$GPM[-train]
  diff[k]=obs-pred
  percdiff[k]=abs(diff[k])/obs
}
me=mean(diff)
rmse=sqrt(mean(diff**2))
mape=100*(mean(percdiff))
me   # mean error
rmse # root mean square error
mape # mean absolute percent error
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;graphs-and-outcomes&#34;&gt;Graphs and Outcomes&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/FuelEfficiency/fueleff_xyplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## regression on all data
m1=lm(GPM~.,data=FuelEff)
summary(m1)
Call:
lm(formula = GPM ~ ., data = FuelEff)

Residuals:
    Min      1Q  Median      3Q     Max
-0.4996 -0.2547  0.0402  0.1956  0.6455

Coefficients:
             Estimate Std. Error t value Pr(&amp;amp;gt;|t|)
(Intercept) -2.599357   0.663403  -3.918 0.000458 ***
WT           0.787768   0.451925   1.743 0.091222 .
DIS         -0.004890   0.002696  -1.814 0.079408 .
NC           0.444157   0.122683   3.620 0.001036 **
HP           0.023599   0.006742   3.500 0.001431 **
ACC          0.068814   0.044213   1.556 0.129757
ET          -0.959634   0.266785  -3.597 0.001104 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.313 on 31 degrees of freedom
Multiple R-squared:  0.9386,    Adjusted R-squared:  0.9267
F-statistic: 78.94 on 6 and 31 DF,  p-value: &amp;lt; 2.2e-16

cor(FuelEff)
           GPM          WT        DIS         NC         HP         ACC         ET
GPM 1.00000000  0.92626656  0.8229098  0.8411880  0.8876992  0.03307093  0.5206121
WT  0.92626656  1.00000000  0.9507647  0.9166777  0.9172204 -0.03357386  0.6673661
DIS 0.82290984  0.95076469  1.0000000  0.9402812  0.8717993 -0.14341745  0.7746636
NC  0.84118805  0.91667774  0.9402812  1.0000000  0.8638473 -0.12924363  0.8311721
HP  0.88769915  0.91722045  0.8717993  0.8638473  1.0000000 -0.25262113  0.7202350
ACC 0.03307093 -0.03357386 -0.1434174 -0.1292436 -0.2526211  1.00000000 -0.3102336
ET  0.52061208  0.66736606  0.7746636  0.8311721  0.7202350 -0.31023357  1.0000000


## best subset regression in R
library(leaps)
X=FuelEff[,2:7]
y=FuelEff[,1]
out=summary(regsubsets(X,y,nbest=2,nvmax=ncol(X)))
tab=cbind(out$which,out$rsq,out$adjr2,out$cp)
tab
  (Intercept) WT DIS NC HP ACC ET
1           1  1   0  0  0   0  0 0.8579697 0.8540244 37.674750
1           1  0   0  0  1   0  0 0.7880098 0.7821212 72.979632
2           1  1   1  0  0   0  0 0.8926952 0.8865635 22.150747
2           1  1   0  0  0   0  1 0.8751262 0.8679906 31.016828
3           1  0   0  1  1   0  1 0.9145736 0.9070360 13.109930
3           1  1   1  1  0   0  0 0.9028083 0.8942326 19.047230
4           1  0   0  1  1   1  1 0.9313442 0.9230223  6.646728
4           1  1   0  1  1   0  1 0.9204005 0.9107520 12.169443
5           1  1   1  1  1   0  1 0.9337702 0.9234218  7.422476
5           1  0   1  1  1   1  1 0.9325494 0.9220103  8.038535
6           1  1   1  1  1   1  1 0.9385706 0.9266810  7.000000

m2=lm(GPM~WT,data=FuelEff)
summary(m2)
Call:
lm(formula = GPM ~ WT, data = FuelEff)

Residuals:
     Min       1Q   Median       3Q      Max
-0.88072 -0.29041  0.00659  0.19021  1.13164

Coefficients:
             Estimate Std. Error t value Pr(&amp;amp;gt;|t|)
(Intercept) -0.006101   0.302681   -0.02    0.984
WT           1.514798   0.102721   14.75   &amp;lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4417 on 36 degrees of freedom
Multiple R-squared:  0.858, Adjusted R-squared:  0.854
F-statistic: 217.5 on 1 and 36 DF,  p-value: &amp;lt; 2.2e-16

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Removing Space of files inside folders/subfolders in bulk</title>
      <link>/post/2019-10-10-remove-space-in-bulk/</link>
      <pubDate>Thu, 10 Oct 2019 23:30:45 +0800</pubDate>
      <guid>/post/2019-10-10-remove-space-in-bulk/</guid>
      <description>&lt;p&gt;In many cases, we need to remove space in the names of files saved inside a tree of folders/subfolders. We can use following CODE to do the job.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:renameNoSpace [/R]
@echo off
setlocal disableDelayedExpansion
if /i &amp;quot;%~1&amp;quot;==&amp;quot;/R&amp;quot; (
set &amp;quot;forOption=%~1 %2&amp;quot;
set &amp;quot;inPath=&amp;quot;
) else (
set &amp;quot;forOption=&amp;quot;
if &amp;quot;%~1&amp;quot; neq &amp;quot;&amp;quot; (set &amp;quot;inPath=%~1\&amp;quot;) else set &amp;quot;inPath=&amp;quot;
)
for %forOption% %%F in (&amp;quot;%inPath%* *&amp;quot;) do (
if /i &amp;quot;%~f0&amp;quot; neq &amp;quot;%%~fF&amp;quot; (
set &amp;quot;folder=%%~dpF&amp;quot;
set &amp;quot;file=%%~nxF&amp;quot;
setlocal enableDelayedExpansion
echo ren &amp;quot;!folder!!file!&amp;quot; &amp;quot;!file: =!&amp;quot;
ren &amp;quot;!folder!!file!&amp;quot; &amp;quot;!file: =!&amp;quot;
endlocal
)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assume the script is called renameNoSpace.bat&lt;/p&gt;

&lt;p&gt;Options to change are&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;renameNoSpace : (no arguments) Renames files in the current directory

renameNoSpace /R : Renames files in the folder tree rooted at the current directory

renameNoSpace myFolder : Renames files in the “myFolder” directory found in the current directory.

renameNoSpace &amp;quot;c:\my folder\&amp;quot; : Renames files in the specified path. Quotes are used because path contains a space.

renameNoSpace /R c:\ : Renames all files on the C: drive.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We paste renameNoSpace.bat to targetted folder/subfolders and run the file in the console.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Orange Juice</title>
      <link>/post/2019-10-09-orange-juice/</link>
      <pubDate>Wed, 09 Oct 2019 23:30:45 +0800</pubDate>
      <guid>/post/2019-10-09-orange-juice/</guid>
      <description>

&lt;p&gt;This is an example presented in the book “Data Mining and Business Analytics with R”, with some useful basic graphs that can be reused for other sets of similar data. Code and Data are saved in &lt;a href=&#34;https://github.com/namkyodai/BusinessAnalytics/tree/master/orangejuice&#34; target=&#34;_blank&#34;&gt;Github link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;store     brand week   logmove feat price     AGE60      EDUC    ETHNIC   INCOME   HHLARGE
1      2 tropicana   40  9.018695    0  3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
2      2 tropicana   46  8.723231    0  3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
3      2 tropicana   47  8.253228    0  3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
4      2 tropicana   48  8.987197    0  3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
5      2 tropicana   50  9.093357    0  3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
6      2 tropicana   51  8.877382    0  3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
7      2 tropicana   52  9.294682    0  3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
8      2 tropicana   53  8.954674    0  3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
9      2 tropicana   54  9.049232    0  3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
10     2 tropicana   57  8.613230    0  3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
11     2 tropicana   58  8.680672    0  3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
12     2 tropicana   59  9.034080    0  3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
13     2 tropicana   60  8.691483    0  3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
14     2 tropicana   61  8.831712    0  3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
15     2 tropicana   62  9.128696    0  3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
16     2 tropicana   63  9.405907    0  2.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
17     2 tropicana   64  9.447150    0  2.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
18     2 tropicana   65  8.783856    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
19     2 tropicana   66  8.723231    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
20     2 tropicana   67  9.957976    0  2.39 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
21     2 tropicana   68  9.426741    0  2.39 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
22     2 tropicana   69  9.156095    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
23     2 tropicana   70  9.793673    0  2.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
24     2 tropicana   71  9.149316    0  2.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
25     2 tropicana   72  8.743851    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
26     2 tropicana   73  8.841014    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
27     2 tropicana   74  9.727228    0  2.49 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
28     2 tropicana   75  8.743851    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
29     2 tropicana   76  8.979165    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
30     2 tropicana   77  8.723231    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
31     2 tropicana   78  8.979165    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
32     2 tropicana   79  8.962904    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
33     2 tropicana   80  8.712760    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
34     2 tropicana   81 10.649607    1  1.69 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
35     2 tropicana   82  8.502689    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
36     2 tropicana   83 10.292281    1  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
37     2 tropicana   84  9.208739    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
38     2 tropicana   85 10.468801    1  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
39     2 tropicana   86 10.083139    0  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
40     2 tropicana   87  8.868413    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
41     2 tropicana   88 10.106918    1  2.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
42     2 tropicana   89  8.754003    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
43     2 tropicana   90  8.712760    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
44     2 tropicana   91 10.420375    0  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
45     2 tropicana   92  9.491602    0  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
46     2 tropicana   93  8.733594    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
47     2 tropicana   94  9.270871    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
48     2 tropicana   95 10.707102    0  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
49     2 tropicana   97  9.908276    0  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
50     2 tropicana   98  9.121728    1  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
51     2 tropicana   99  9.996614    0  2.19 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
52     2 tropicana  100  9.515469    0  2.19 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
53     2 tropicana  103  8.333270    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
54     2 tropicana  104 10.582130    1  1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
55     2 tropicana  105  8.636220    0  3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
56     2 tropicana  106  9.107643    1  2.68 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
57     2 tropicana  107  8.702178    0  3.44 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
58     2 tropicana  108  8.954674    0  3.14 0.2328647 0.2489349 0.1142799 10.55321 0.1039534
     WORKWOM   HVAL150 SSTRDIST  SSTRVOL CPDIST5   CPWVOL5
1  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
2  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
3  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
4  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
5  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
6  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
7  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
8  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
9  0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
10 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
11 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
12 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
13 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
14 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
15 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
16 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
17 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
18 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
19 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
20 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
21 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
22 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
23 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
24 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
25 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
26 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
27 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
28 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
29 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
30 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
31 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
32 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
33 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
34 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
35 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
36 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
37 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
38 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
39 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
40 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
41 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
42 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
43 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
44 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
45 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
46 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
47 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
48 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
49 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
50 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
51 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
52 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
53 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
54 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
55 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
56 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
57 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
58 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266
 [ reached &#39;max&#39; / getOption(&amp;quot;max.print&amp;quot;) -- omitted 28889 rows ]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Install packages from CRAN; use any USA mirror
library(lattice)
#oj &amp;lt;- read.csv(&amp;quot;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/oj.csv&amp;quot;)
oj &amp;lt;- read.csv(&amp;quot;oj.csv&amp;quot;)
oj$store &amp;lt;- factor(oj$store) #change numberic value of store into categorical data
oj[1:2,]
t1=tapply(oj$logmove,oj$brand,FUN=mean,na.rm=TRUE) #calculate the mean of each brand using logmove value.
t1
t2=tapply(oj$logmove,INDEX=list(oj$brand,oj$week),FUN=mean,na.rm=TRUE) #calculate the mean of logmove value based on index lists per week.
t2
#plot each graph as time serieas data per week.
plot.new()
par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2),bg=&amp;quot;white&amp;quot;,cex = 1, cex.main = 0.6)
plot(t2[1,],type= &amp;quot;l&amp;quot;,xlab=&amp;quot;week&amp;quot;,ylab=&amp;quot;dominicks&amp;quot;,ylim=c(7,12),cex.axis = 1,las = 1)
plot(t2[2,],type= &amp;quot;l&amp;quot;,xlab=&amp;quot;week&amp;quot;,ylab=&amp;quot;minute.maid&amp;quot;,ylim=c(7,12),cex.axis = 1,las = 1)
plot(t2[3,],type= &amp;quot;l&amp;quot;,xlab=&amp;quot;week&amp;quot;,ylab=&amp;quot;tropicana&amp;quot;,ylim=c(7,12),cex.axis = 1,las = 1)
dev.copy(png,&#39;oj_weekmean01.png&#39;,width = 1600, height = 600)
dev.off()
#-------------------------------
#now we combine the three above graphs into one single graphs for ease of comparison
logmove=c(t2[1,],t2[2,],t2[3,])
week1=c(40:160)
week=c(week1,week1,week1)
brand1=rep(1,121)
brand2=rep(2,121)
brand3=rep(3,121)
brand=c(brand1,brand2,brand3)
plot.new()
xyplot(logmove~week|factor(brand),type= &amp;quot;l&amp;quot;,layout=c(1,3),col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_weekmean02.png&#39;,width = 1000, height = 600)
dev.off()
#-----------------------------
plot.new()
par(mfrow=c(1,1))
#par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2),bg=&amp;quot;white&amp;quot;,cex = 1, cex.main = 0.6)
boxplot(logmove~brand,data=oj) # compare logmove of 3 branch using boxplot
dev.copy(png,&#39;oj_logmovebrandboxplot.png&#39;,width = 800, height = 600)
dev.off()
histogram(~logmove|brand,data=oj,layout=c(1,3)) # compare logmove of 3 branch using histogram
dev.copy(png,&#39;oj_logmovebrandhist.png&#39;,width = 1000, height = 600)
dev.off()
a1=densityplot(~logmove|brand,data=oj,layout=c(1,3),plot.points=FALSE) # compare logmove of 3 branch using density plot
a2=densityplot(~logmove,groups=brand,data=oj,plot.points=FALSE) ## compare logmove of 3 branch using density plot in a one frame
#using xyplot to see the spartial distribution of data weekly
library(gridExtra) #this package allows to plot multiple graphs in the same plot despite the difference in plotting engines (e.g. ggplot or barchart)
grid.arrange(a1, a2,  ncol = 2) #display the two plot a and p
dev.copy(png,&#39;oj_logmovedensity.png&#39;,width = 1000, height = 500)
dev.off()

#-------------------------------------------------
xyplot(logmove~week,data=oj,col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_logmoveweekspartial.png&#39;,width = 1000, height = 500)
dev.off()

#---------------------------------
xyplot(logmove~week|brand,data=oj,layout=c(1,3),col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_logmoveweekspartialbrand.png&#39;,width = 1000, height = 500)
dev.off()

#---------------------------------
xyplot(logmove~price,data=oj,col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_logmoveprice.png&#39;,width = 1000, height = 500)
dev.off()

#---------------------------------
xyplot(logmove~price|brand,data=oj,layout=c(1,3),col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_logmovepricebrand.png&#39;,width = 1000, height = 500)
dev.off()

#---------------------------------
smoothScatter(oj$price,oj$logmove)
dev.copy(png,&#39;oj_logmovepricesmooth.png&#39;,width = 1000, height = 500)
dev.off()
#---------------------------------
a1=densityplot(~logmove,groups=feat, data=oj, plot.points=FALSE)
a2=xyplot(logmove~price,groups=feat, data=oj)
grid.arrange(a1, a2,  ncol = 2) #display the two plot a and p
dev.copy(png,&#39;oj_logmovepricegroupfeat.png&#39;,width = 1200, height = 500)
dev.off()

#------------------------------------------------
oj1=oj[oj$store == 5,]
xyplot(logmove~week|brand,data=oj1,type=&amp;quot;l&amp;quot;,layout=c(1,3),col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_logmovebrand.png&#39;,width = 1200, height = 500)
dev.off()

xyplot(logmove~price,data=oj1,col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_logmovepricexyplot.png&#39;,width = 800, height = 500)
dev.off()


xyplot(logmove~price|brand,data=oj1,layout=c(1,3),col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;oj_logmovepricebrandxyplot.png&#39;,width = 1000, height = 500)
dev.off()

densityplot(~logmove|brand,groups=feat,data=oj1,plot.points=FALSE)
dev.copy(png,&#39;oj_logmovebranddenst.png&#39;,width = 1200, height = 500)
dev.off()

xyplot(logmove~price|brand,groups=feat,data=oj1)
dev.copy(png,&#39;oj_logmovepricebrandxyplot.png&#39;,width = 1200, height = 500)
dev.off()

#----------------------------
t21=tapply(oj$INCOME,oj$store,FUN=mean,na.rm=TRUE)
t21
t21[t21==max(t21)]
t21[t21==min(t21)]

oj1=oj[oj$store == 62,]
oj2=oj[oj$store == 75,]
oj3=rbind(oj1,oj2)

#----------------------------------------
a1=xyplot(logmove~price|store,data=oj3)
a2=xyplot(logmove~price|store,groups=feat,data=oj3)
grid.arrange(a1, a2,  ncol = 1) #display the two plot a and p
dev.copy(png,&#39;oj_logmovexyplotprice.png&#39;,width = 500, height = 1000)
dev.off()

## store in the wealthiest neighborhood
plot.new()
par(mar=c(4,4,1,1)+0.1,mfrow=c(1,2),bg=&amp;quot;white&amp;quot;,cex = 1, cex.main = 1)
mhigh=lm(logmove~price,data=oj1)
summary(mhigh)
plot(logmove~price,data=oj1,xlim=c(0,4),ylim=c(0,13), main=&amp;quot;62 = wealthiest store&amp;quot;)
abline(mhigh)
## store in the poorest neighborhood
mlow=lm(logmove~price,data=oj2)
summary(mlow)
plot(logmove~price,data=oj2,xlim=c(0,4),ylim=c(0,13), main=&amp;quot;75 = poorest store&amp;quot;)
abline(mlow)
dev.copy(png,&#39;oj_logmovepriceoj2.png&#39;,width = 1000, height = 300)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;graphs&#34;&gt;Graphs&lt;/h1&gt;

&lt;p&gt;Mean of logmove over 121 weeks&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_weekmean01.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_weekmean02.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;boxplot&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovebrandboxplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovebrandhist.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovedensity.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmoveweekspartial.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmoveweekspartialbrand.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmoveprice.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovebrand.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovepricebrand.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovepricesmooth.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovepricegroupfeat.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovebranddenst.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovepricebrandxyplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovepricexyplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovexyplotprice.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/orangejuice/oj_logmovepriceoj2.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The last two graphs also present the regression lines with summary of coefficients using lm function&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mhigh=lm(logmove~price,data=oj1)
summary(mhigh)

Call:
lm(formula = logmove ~ price, data = oj1)

Residuals:
    Min      1Q  Median      3Q     Max
-4.9557 -0.4934  0.1815  0.6557  2.4454

Coefficients:
            Estimate Std. Error t value Pr(&amp;amp;gt;|t|)
(Intercept)  9.15394    0.21112  43.359   &amp;lt;2e-16 ***
price       -0.01461    0.08381  -0.174    0.862
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.142 on 349 degrees of freedom
Multiple R-squared:  8.712e-05, Adjusted R-squared:  -0.002778
F-statistic: 0.03041 on 1 and 349 DF,  p-value: 0.8617


mlow=lm(logmove~price,data=oj2)
summary(mlow)

Call:
lm(formula = logmove ~ price, data = oj2)

Residuals:
    Min      1Q  Median      3Q     Max
-3.5235 -0.5606  0.0392  0.5090  2.4523

Coefficients:
            Estimate Std. Error t value Pr(&amp;amp;gt;|t|)
(Intercept) 10.87695    0.15184   71.63   &amp;lt;2e-16 ***
price       -0.67222    0.06071  -11.07   &amp;lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.8383 on 352 degrees of freedom
Multiple R-squared:  0.2584,    Adjusted R-squared:  0.2563
F-statistic: 122.6 on 1 and 352 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Alumni Contributions</title>
      <link>/post/2019-10-08-alumni-contribution/</link>
      <pubDate>Tue, 08 Oct 2019 23:30:45 +0800</pubDate>
      <guid>/post/2019-10-08-alumni-contribution/</guid>
      <description>&lt;p&gt;This is a summary of example 2 in Chapter 2 of the book “Data Mining and Business Analytics with R”. The post keeps the original code with some polishing syntax for better plotting, particularly using ggplot2 package and gridExtra.&lt;/p&gt;

&lt;p&gt;Code and Data are saved in &lt;a href=&#34;https://github.com/namkyodai/BusinessAnalytics/tree/master/AlumniContributions&#34; target=&#34;_blank&#34;&gt;Github link&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(lattice)
library(ggplot2)

#Using multiple plot function when using with ggplot
#source(&amp;quot;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/genericfunctions/multiplot.R&amp;quot;)


#----1. Data
don &amp;lt;- read.csv(&amp;quot;contribution.csv&amp;quot;)
#or read directly from the web
#don &amp;lt;- read.csv(&amp;quot;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/contribution.csv&amp;quot;)
#or

don[1:5,] #display the first 5 data rows

table(don$Class.Year) #display total numbers of data points for each batch of year
a=barchart(table(don$Class.Year),horizontal=FALSE,xlab=&amp;quot;Class Year&amp;quot;,col=&amp;quot;black&amp;quot;)
p=ggplot(data.frame(table(don$Class.Year)), aes(x=Var1, y=Freq))+labs(y=&amp;quot;Freq&amp;quot;, x=&amp;quot;Class Year&amp;quot;) + geom_bar(stat=&amp;quot;identity&amp;quot;,width=0.8,color=&amp;quot;blue&amp;quot;,fill=&amp;quot;steelblue&amp;quot;)+geom_text(aes(label=Freq), vjust=-0.3, size=3.5)
plot.new()
par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(1,2),bg=&amp;quot;white&amp;quot;)
library(gridExtra) #this package allows to plot multiple graphs in the same plot despite the difference in plotting engines (e.g. ggplot or barchart)
grid.arrange(a, p, ncol = 2) #display the two plot a and p

dev.copy(png,&#39;alumni_classyear_bar.png&#39;,width = 1200, height = 500)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; don[1:5,]
  Gender Class.Year Marital.Status   Major Next.Degree FY04Giving FY03Giving FY02Giving
1      M       1957              M History         LLB       2500       2500       1400
2      M       1957              M Physics          MS       5000       5000       5000
3      F       1957              M   Music        NONE       5000       5000       5000
4      M       1957              M History        NONE          0       5100        200
5      M       1957              M Biology          MD       1000       1000       1000
  FY01Giving FY00Giving AttendenceEvent
1      12060      12000               1
2       5000      10000               1
3       5000      10000               1
4        200          0               1
5       1005       1000               1
&amp;gt; table(don$Class.Year)

1957 1967 1977 1987 1997
 127  222  243  277  361
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Barchart from Lattice package&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_classyear_bar.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;don$TGiving=don$FY00Giving+don$FY01Giving+don$FY02Giving+don$FY03Giving+don$FY04Giving
mean(don$TGiving)
sd(don$TGiving)
quantile(don$TGiving,probs=seq(0,1,0.05))
quantile(don$TGiving,probs=seq(0.95,1,0.01))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(don$TGiving)
[1] 980.0436
&amp;gt; sd(don$TGiving)
[1] 6670.773
&amp;gt; quantile(don$TGiving,probs=seq(0,1,0.05))
      0%       5%      10%      15%      20%      25%      30%      35%      40%      45%
     0.0      0.0      0.0      0.0      0.0      0.0      0.0     10.0     25.0     50.0
     50%      55%      60%      65%      70%      75%      80%      85%      90%      95%
    75.0    100.0    150.8    200.0    275.0    400.0    554.2    781.0   1050.0   2277.5
    100%
171870.1
&amp;gt; quantile(don$TGiving,probs=seq(0.95,1,0.01))
      95%       96%       97%       98%       99%      100%
  2277.50   3133.56   5000.00   7000.00  16442.14 171870.06
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#---------------------
plot.new()
par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2))
hist(don$TGiving,main=NULL,xlab=&amp;quot;Total Contribution&amp;quot;) #histograph with outliners
hist(don$TGiving[don$TGiving!=0][don$TGiving[don$TGiving!=0]&amp;lt;=1000],main=NULL,xlab=&amp;quot;Total Contribution&amp;quot;) #histograph after delete outliners
boxplot(don$TGiving,horizontal=TRUE,xlab=&amp;quot;Total Contribution&amp;quot;) #boxplot with outliners
boxplot(don$TGiving,outline=FALSE,horizontal=TRUE,xlab=&amp;quot;Total Contribution&amp;quot;) #boxplot without outliners
dev.copy(png,&#39;alumni_contributionplot.png&#39;,width = 800, height = 500)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_contributionplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ddd=don[don$TGiving&amp;gt;=30000,] #seeing only total giving greater than 30K
ddd
ddd1=ddd[,c(1:5,12)] #display colum from 1 to 5 and column 12
ddd1
ddd1[order(ddd1$TGiving,decreasing=TRUE),] #display with decreasing

#-----------------
plot.new()
par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2))
boxplot(TGiving~Class.Year,data=don,outline=FALSE, xlab=&amp;quot;year&amp;quot;)
boxplot(TGiving~Gender,data=don,outline=FALSE, xlab=&amp;quot;sex&amp;quot;)
boxplot(TGiving~Marital.Status,data=don,outline=FALSE,xlab=&amp;quot;Marital status&amp;quot;)
boxplot(TGiving~AttendenceEvent,data=don,outline=FALSE,xlab=&amp;quot;Attend event or not&amp;quot;)

dev.copy(png,&#39;alumni_distribution_boxplot.png&#39;,width = 800, height = 500)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_distribution_boxplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot.new()
#-----------------
t4=tapply(don$TGiving,don$Major,mean,na.rm=TRUE)
t4
t5=table(don$Major)
t5
t6=cbind(t4,t5)
t7=t6[t6[,2]&amp;gt;10,]
t7[order(t7[,1],decreasing=TRUE),]
plot(barchart(t7[,1],col=&amp;quot;black&amp;quot;))
dev.copy(png,&#39;alumni_major_barplot.png&#39;,width = 800, height = 500)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_major_barplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#-----------------
plot.new()
t4=tapply(don$TGiving,don$Next.Degree,mean,na.rm=TRUE)
t4
t5=table(don$Next.Degree)
t5
t6=cbind(t4,t5)
t7=t6[t6[,2]&amp;gt;10,]
t7[order(t7[,1],decreasing=TRUE),]
plot(barchart(t7[,1],col=&amp;quot;black&amp;quot;))
dev.copy(png,&#39;alumni_degree_barplot.png&#39;,width = 800, height = 500)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_degree_barplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#-----------------
plot.new()
densityplot(~TGiving|factor(Class.Year),data=don[don$TGiving&amp;lt;=1000,][don[don$TGiving&amp;lt;=1000,]$TGiving&amp;amp;gt;0,],plot.points=FALSE,col=&amp;quot;black&amp;quot;)
dev.copy(png,&#39;alumni_year_densityplot.png&#39;,width = 800, height = 500)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_degree_densityplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t11=tapply(don$TGiving,don$Class.Year,FUN=sum,na.rm=TRUE)
t11
#-----------------
plot.new()
par(mfrow=c(1,1))
barplot(t11,ylab=&amp;quot;Average Donation&amp;quot;)
dev.copy(png,&#39;alumni_year_barplot.png&#39;,width = 800, height = 500)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_year_barplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#-----------------
plot.new()
par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2))
barchart(tapply(don$FY04Giving,don$Class.Year,FUN=sum,
                 na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2004&amp;quot;)

barchart(tapply(don$FY03Giving,don$Class.Year,FUN=sum,
                 na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2003&amp;quot;)
barchart(tapply(don$FY02Giving,don$Class.Year,FUN=sum,
                 na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2002&amp;quot;)
barchart(tapply(don$FY01Giving,don$Class.Year,FUN=sum,
                 na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2001&amp;quot;)
barchart(tapply(don$FY00Giving,don$Class.Year,FUN=sum,
                 na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2000&amp;quot;)

#same plot but with par
#-----------------
plot.new()
par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(3,2),bg=&amp;quot;white&amp;quot;)
barplot(tapply(don$FY04Giving,don$Class.Year,FUN=sum,
                na.rm=TRUE),ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2004&amp;quot;)
barplot(tapply(don$FY03Giving,don$Class.Year,FUN=sum,
                na.rm=TRUE),ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2003&amp;quot;)
barplot(tapply(don$FY02Giving,don$Class.Year,FUN=sum,
                na.rm=TRUE),ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2002&amp;quot;)
barplot(tapply(don$FY01Giving,don$Class.Year,FUN=sum,
                na.rm=TRUE),ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2001&amp;quot;)
barplot(tapply(don$FY00Giving,don$Class.Year,FUN=sum,
                na.rm=TRUE),ylim=c(0,225000),col=&amp;quot;black&amp;quot;, main=&amp;quot;2000&amp;quot;)

dev.copy(png,&#39;alumni_annual_barplot.png&#39;,width = 500, height = 800)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_barplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#-----------------
plot.new()
par(mfrow=c(1,1))

don$TGivingIND=cut(don$TGiving,breaks=c(-1,0.5,10000000),labels=FALSE)-1
mean(don$TGivingIND)
t5=table(don$TGivingIND,don$Class.Year)
t5
barplot(t5,beside=TRUE)
mosaicplot(factor(don$Class.Year)~factor(don$TGivingIND))
t50=tapply(don$TGivingIND,don$Class.Year,FUN=mean,na.rm=TRUE)
t50
p3=barchart(t50,horizontal=FALSE,xlab=&amp;quot;Class Year&amp;quot;,col=&amp;quot;black&amp;quot;, main=&amp;quot;TGiving&amp;quot;)
don$FY04GivingIND=cut(don$FY04Giving,c(-1,0.5,10000000),labels=FALSE)-1
t51=tapply(don$FY04GivingIND,don$Class.Year,FUN=mean,na.rm=TRUE)
t51
p4=barchart(t51,horizontal=FALSE,xlab=&amp;quot;Class Year&amp;quot;,col=&amp;quot;black&amp;quot;, main=&amp;quot;FY04Giving&amp;quot;)
grid.arrange(p3, p4,  ncol = 2)
dev.copy(png,&#39;alumni_annual_barplotfreq.png&#39;,width = 800, height = 300)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_barplotbeside.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_mosaicplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_barplotfreq.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_correlatonplot.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_correlatonploteclipse.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_correlatonplotpie.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/AlumniContributions/alumni_annual_genderattend01.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Earned Value Management</title>
      <link>/post/2019-10-07-evm/</link>
      <pubDate>Mon, 07 Oct 2019 23:30:45 +0800</pubDate>
      <guid>/post/2019-10-07-evm/</guid>
      <description>

&lt;p&gt;Earned Value Management (EVM) is a widely used method, if not to say, a imperative method in project management. Without the EVM, the Project Managers and stakeholders (PM) cannot track and monitor the Progress and Schedule, cannot understand the causes of problems and delays.&lt;/p&gt;

&lt;p&gt;In a large scale engineering project, Contractors shall have an excellent and FULL TIME experienced Planners who can confidently perform EVM analysis and report to the PM and the project team weekly and monthly. The analysis report shall includes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Updated Project Schedule with actual progress;&lt;/li&gt;
&lt;li&gt;Comparison between the Actual Progress vs Baselines;&lt;/li&gt;
&lt;li&gt;Critical Path Analysis (CPA) and identification of factors contributing to especially delays;
The EVM.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/2/20/EarnedValueChartNormalized.jpg&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It seems that creating the graph like the one shown above is EASY. ^_^ Nope, it is not easy at all, but if it is not difficult if Planners understand the statistics and a smart way to work with compiling data.&lt;/p&gt;

&lt;p&gt;This article presents 2 different ways to generate the EVM graphs.&lt;/p&gt;

&lt;h1 id=&#34;evm-for-consultancy-project-using-ms-project-and-r&#34;&gt;EVM for Consultancy Project using MS Project and R&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Coded by Nam Le (namlt@prontonmail.ch)
library(scales)
library(reshape2)
library(ggplot2)

#----------DATA-----------------
data &amp;lt;- read.csv(file=&amp;quot;Scurve.csv&amp;quot;, header=TRUE, sep=&amp;quot;,&amp;quot;)
k=data.frame(data)

# -----------------------------This needs to be modified whenever you plot

plot.new()
par(mar=c(4, 4, 4, 4), bg=&amp;quot;ivory&amp;quot;)
week= length(data[!is.na(data[,3]),3]) #the lastest week that EV value is available. Or you can simply put the number of week in instead of using length(data[!is.na(data[,3]),3]). Here 3 is the column.
scalefactor=100
trucy=100
ticktime=35   #Set the Project Week at which you want to limit the X-axis
varrange=c(-30,30)

#Posion of texbox (PV, EV, and VAR in the graph)
htextbox=c(week-5,week)
vtextbox=20
 # --------------------------------
 # ----------------------------
#Plot the baseline

plot(data$BS01[1:ticktime],pch=4,axes=FALSE,ylim=c(0,trucy),ylab=&amp;quot;&amp;quot;, xlab=&amp;quot;&amp;quot;,col=&amp;quot;blue&amp;quot;,type=&amp;quot;o&amp;quot;,lwd=1,lty=1, main =paste(&amp;quot;S-curve of week&amp;quot;, week))
axis(2, ylim=c(0,trucy),col=&amp;quot;darkblue&amp;quot;,las=1)
mtext(expression(paste(&amp;quot;Cummulative percentage (%)&amp;quot;)),side=2,line=2.2, adj = 0.5 )
axis(1,pretty(range(data$Week),30))
mtext(expression(paste(&amp;quot;Project Week&amp;quot;)),side=1,col=&amp;quot;black&amp;quot;,line=2.2)
box()

#plot the EV curve
par(new=TRUE)
plot(data$EV[1:ticktime],pch=18,axes=FALSE,ylim=c(0,trucy),ylab=&amp;quot;&amp;quot;, xlab=&amp;quot;&amp;quot;,col=&amp;quot;red&amp;quot;,type=&amp;quot;o&amp;quot;,lwd=1,lty=2)

#adding stick at the actual week
abline(v=week, col=&amp;quot;darkviolet&amp;quot;,lty=3)

par(new=TRUE)
#library(gplots)
library(astro) #this package is for astronomy but it has some fantastic function for plotting. Here I use the textbox function
textbox(htextbox, vtextbox, textlist=c(paste(&amp;quot;PV  =&amp;quot;,format(round(data$BS01[week],2)),&amp;quot;EV  =&amp;quot;,format(round(data$EV[week],2)),&amp;quot;VAR=&amp;quot;,format(round(data$EV[week]-data$BS01[week],2)))), justify=&#39;f&#39;, cex=0.7,col=&amp;quot;purple&amp;quot;, font=2, border=&amp;quot;green&amp;quot;, margin=-0.025,adj=0,box=1,fill=&amp;quot;aliceblue&amp;quot;)
#alternatively, we can also use legend function for the same purpose, but I believe textbox function gives better look.

#legend(30, 50, c(paste(&amp;quot;PV:&amp;quot;),format(round(data$BS01[week],2)),paste(&amp;quot;EV:&amp;quot;),format(round(data$EV[week],2)), paste(&amp;quot;VAR:&amp;quot;),format(round(data$BS01[week]-data$EV[week],2))), col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;),cex = 0.6)

 #plot the VAR curve using
par(new=TRUE)
plot((data$EV[1:ticktime]-data$BS01[1:ticktime]),pch=1,axes=FALSE,ylim=varrange,ylab=&amp;quot;&amp;quot;, xlab=&amp;quot;&amp;quot;,col=&amp;quot;cadetblue4&amp;quot;,type=&amp;quot;o&amp;quot;,lwd=1,lty=3)
axis(4, ylim=varrange,col=&amp;quot;darkblue&amp;quot;,las=1)
mtext(expression(paste(&amp;quot;Variance (%)&amp;quot;)),side=4,line=2.2, adj = 0.5  )
abline(h=0, col=&amp;quot;darkviolet&amp;quot;,lty=3)

 #plot the AC curve: This curve is added only for internal monitoring purpose. Shall not give this to the other stakeholders.
par(new=TRUE)
plot(data$AC[1:ticktime],pch=2,axes=FALSE,ylim=c(0,trucy),ylab=&amp;quot;&amp;quot;, xlab=&amp;quot;&amp;quot;,col=&amp;quot;violetred3&amp;quot;,type=&amp;quot;o&amp;quot;,lwd=1,lty=1)
#axis(4, ylim=varrange,col=&amp;quot;darkblue&amp;quot;,las=1)
#mtext(expression(paste(&amp;quot;Variance (%)&amp;quot;)),side=4,line=2.2, adj = 0.5  )
#abline(h=0, col=&amp;quot;darkviolet&amp;quot;,lty=3)

#adding Planned Value, Earned Value, and Variance
#adding legend
legend(&amp;quot;topleft&amp;quot;, c(&amp;quot;PV&amp;quot;,&amp;quot;EV&amp;quot;, &amp;quot;VAR&amp;quot;, &amp;quot;AC&amp;quot;), text.col
=&amp;quot;red&amp;quot;, border = &amp;quot;white&amp;quot;,box.lwd = 1,bg=&amp;quot;aliceblue&amp;quot;,lwd = 1,pch=c(4,18,1,2),lty =c(1,2,3,1),  col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;cadetblue4&amp;quot;,&amp;quot;violetred3&amp;quot;),inset = .05,cex=0.8)

#save png file
dev.copy(png,&#39;evm.png&#39;,width = 800, height = 500)
dev.off()
#-----THE END------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/EVM/evm.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Return on Investment (ROI) for Intervention of Pumps under uncertainties</title>
      <link>/post/2019-09-29-pump-efficiency/</link>
      <pubDate>Sun, 29 Sep 2019 23:30:45 +0800</pubDate>
      <guid>/post/2019-09-29-pump-efficiency/</guid>
      <description>

&lt;p&gt;This post describes a generic approach on the computation of Net Present Value (NPV) and Life Cycle Cost estimation (LCC) for Preventive Intervention (PI) on pumps (e.g. centrifugal pumps) when decision makers are uncertain on, or do not have a rich set of data on failure of pump components.&lt;/p&gt;

&lt;p&gt;Ideally, failure data on pump components should be recorded in time-series fashion that allows analyst to investigate on the frequency of failure and come up with a prediction. The prediction is powerful in view of generating budget for purchasing spare parts for not only one pump station but also for the entire network of pump stations. This is so-called Integrated Asset Management approach that should be at the center of an organization who has a long term view on how to sustain and prolong their assets.&lt;/p&gt;

&lt;p&gt;However, in many practical situations, failure data is limited or recorded inappropriately that does not become a useful set of information for analytic. In such a circumstance, decisions on whether to replace/rehabilitate pumps should be dependent on various uncertainty factors such as efficiency, assumption on average decaying rate of the assets, and most importantly the energy consumption vs the production produce.&lt;/p&gt;

&lt;p&gt;This article presents a simple yet useful model for managers to make decisions on replacement of pumps if they are more or less aware of deficiency, low efficiency, and high ratio of energy and production over time.&lt;/p&gt;

&lt;p&gt;Aside from the above, it is also worth to consider replacement of pumps when the two following aspects are existed.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Asset obsolescence;&lt;/li&gt;
&lt;li&gt;Technological innovation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There two aspects should be viewed in a combination. Asset obsolescence could be a result of technological innovation or changes in standards and requirements. Suffice it to state that pumps of a station were designed/installed more than a decade ago, nowadays, pumps are designed and manufactured to be more reliable and durable, whilst the capital cost is significant lower than that in the past.&lt;/p&gt;

&lt;h1 id=&#34;the-model&#34;&gt;The model&lt;/h1&gt;

&lt;p&gt;Total Cost (TC) incurred in a period of T years is defined as the summation of Capital Cost &amp;copy; and Operation cost (O). It is note that operational cost includes routine maintenance cost.&lt;/p&gt;

&lt;p&gt;$$ TC(T) = \sum&lt;em&gt;{i=1}^{I} \sum&lt;/em&gt;{t=1}^{T}\frac{C&lt;em&gt;{i,t}+O&lt;/em&gt;{i,t}\times \epsilon&lt;em&gt;{i,t}^{new}\times u&lt;/em&gt;{i,t}+(1-\delta&lt;em&gt;{i,t})\times O&lt;/em&gt;{i,t}\epsilon&lt;em&gt;{i,t}^{old}\times u&lt;/em&gt;{i,t}}{(1+\rho)^t}$$&lt;/p&gt;

&lt;p&gt;In the equation,  $\epsilon&lt;em&gt;{i,t}$, $u&lt;/em&gt;{i,t}$, and  $\delta_{i,t}$ are the decreasing rate of efficiency, utilization, and decision variable of pump i in year t, respectively.&lt;/p&gt;

&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;

&lt;p&gt;A pump station has 6 booster pumps with the same configuration of 500 horsepower. The station has been in commission for more than 10 years and there has been a progressive degradation process, though it has not been captured appropriately. Sufficient failure data at component level is not available.&lt;/p&gt;

&lt;p&gt;Tests have been conducted to measure the flow and the power rating that allows to come up efficiency of pump. However, due to non-optimal pipe configuration, the obtained efficiencies are also subjected to uncertainties.&lt;/p&gt;

&lt;p&gt;Under such circumstance, decision makers have to make decisions under uncertainties.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Discount factor is 8.5% annually;&lt;/li&gt;
&lt;li&gt;Capital cost for buying/installing a new pump of 500 horsepower would cost 5 millions Peso (~10,000 usd);&lt;/li&gt;
&lt;li&gt;Efficiency of existing pumps are about 5% less than that of the original design;&lt;/li&gt;
&lt;li&gt;Pumps are assumed to operate at 98% utilization level;&lt;/li&gt;
&lt;li&gt;Price of 1 KW is 6.5 Peso;&lt;/li&gt;
&lt;li&gt;There are 2 Intervention Strategies (IS) to be considerred. Herein refer to TC1 and TC2, respectively. TC1 is to replace existing pump with a new one in a step of 1 year. TC2 is Do Nothing, i.e., to keep the existing pumps as they are.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;source-code&#34;&gt;Source Code&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#this subroutine is used to estimate the cashflow of an investment for pump stations

Npumps = 6 #total number of pumps
TPeriod=10 #this infers 5 years
TC1&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #Option 1
TC2&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #Option 2
CAPEX&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #Captical investment cost
OPEX&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #Operational Cost
delta&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency
epsilonold&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency
epsilonnew&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency
u&amp;lt;-matrix(nrow=TPeriod,ncol = Npumps) #ultilization
rho=0.085 #discount factor
pricee=6.5
hoursepower=500
utilization=0.98
Newpumpcost=5000000

#Option 1 - Stagging investment for pump

for (t in 1: TPeriod){
  if (t&amp;lt;2){
    for (i in 1:Npumps){
      epsilonold[t,i]&amp;lt;-1.05
    }
  } else {
    for (i in 1:Npumps){
    epsilonold[t,i]&amp;lt;-epsilonold[t-1,i]*1.05
    }
  }
}
for (t in 1:TPeriod){
  for (i in 1:Npumps){
    delta[,]&amp;lt;-0
    delta[1,1]&amp;lt;-1
    epsilonnew[1,1]&amp;lt;-1
    delta[2,2]&amp;lt;-1
    epsilonnew[2,2]&amp;lt;-1
    delta[3,3]&amp;lt;-1
    epsilonnew[3,3]&amp;lt;-1
    delta[4,4]&amp;lt;-1
    epsilonnew[4,4]&amp;lt;-1
    delta[5,5]&amp;lt;-1
    epsilonnew[5,5]&amp;lt;-1
    delta[6,6]&amp;lt;-1
    epsilonnew[6,6]&amp;lt;-1
    CAPEX[t,i]&amp;lt;-Newpumpcost #peso
    u[t,i]&amp;lt;-utilization
    OPEX[t,i]&amp;lt;- pricee*hoursepower*365*24*0.746*u[t,i] #peso
      }
}
for (t in 1: TPeriod){
  for (i in 1:Npumps){
    if (delta[t,i]==1){

      epsilonnew[t,i]&amp;lt;-1

    } else if (i==1 &amp;amp;amp; t==1){

        epsilonnew[t,i]&amp;lt;-1


    } else if (t&amp;amp;gt;1){
      epsilonnew[t,i]&amp;lt;-epsilonnew[t-1,i]*1
    }

  }

}

epsilonnew[is.na(epsilonnew)] &amp;lt;- 0

#redefine the value of epsilonold based on new value of epsilonnew


for (t in 1: TPeriod){
  for (i in 1:Npumps){
    if (epsilonnew[t,i]&amp;lt;1){

      epsilonold[t,i]&amp;lt;-epsilonold[t,i]

    } else if (epsilonnew[t,i]==1){
       epsilonold[t,i]&amp;lt;-0
     }
   }
 }


for (i in 1:Npumps){
  for (t in 1:TPeriod){
    TC1[t,i]&amp;lt;-(((CAPEX[t,i]*delta[t,i]+OPEX[t,i]*epsilonnew[t,i]))+(1-delta[t,i])*OPEX[t,i]*epsilonold[t,i])/(1+rho)**t
  }
}
cat(&amp;quot;Replacement of pumps \n&amp;quot;)

print(TC1)

#stop(&amp;quot;dd&amp;quot;)

#Option 2 - Do Nothing

for (t in 1:TPeriod){
  for (i in 1:Npumps){
    delta[,]&amp;lt;-0
    CAPEX[t,i]&amp;lt;-0 #peso
    u[t,i]&amp;lt;-utilization
    epsilonnew[t,i]&amp;lt;-1.0
      OPEX[t,i]&amp;lt;- pricee*hoursepower*365*24*0.746*u[t,i] #peso
  }
}

for (t in 1: TPeriod){
  if (t&amp;lt;2){
    for (i in 1:Npumps){
      epsilonold[t,i]&amp;lt;-1.05
    }
  } else {
    for (i in 1:Npumps){
      epsilonold[t,i]&amp;lt;-epsilonold[t-1,i]*1.05
    }
  }
}


for (i in 1:Npumps){
  for (t in 1:TPeriod){
    TC2[t,i]&amp;lt;-(((CAPEX[t,i]+OPEX[t,i]*epsilonnew[t,i])*delta[t,i])+(1-delta[t,i])*OPEX[t,i]*epsilonold[t,i])/(1+rho)**t
  }
}

cat(&amp;quot;Do Nothing \n&amp;quot;)
print(TC2)

cat(&amp;quot;The difference of investment \n&amp;quot;)
print(TC1-TC2)

#plot the graph for comparison

library(ggplot2)
time=c(1:TPeriod)

x&amp;lt;-data.frame(dat=TC1[,1],IS=rep(&amp;quot;TC1&amp;quot;))
y&amp;lt;-data.frame(dat=TC2[,1],IS=rep(&amp;quot;TC2&amp;quot;))

x&amp;lt;-cbind(time,x)
y&amp;lt;-cbind(time,y)
 xy&amp;lt;- rbind(x, y)
ggplot(xy, aes(fill=IS, y=dat, x=factor(time))) + geom_bar(position=&amp;quot;dodge&amp;quot;, stat=&amp;quot;identity&amp;quot;)
stop(&amp;quot;&amp;quot;)

TC1&amp;lt;-data.frame(TC1,IS=rep(&amp;quot;TC1&amp;quot;))
TC2&amp;lt;-data.frame(TC2,IS=rep(&amp;quot;TC2&amp;quot;))
#TC2&amp;lt;-data.frame(TC2=TC2[,1])

#TC1&amp;lt;-data.frame(TC1)
#TC2&amp;lt;-data.frame(TC2)

TC1&amp;lt;-cbind(time,TC1)
TC2&amp;lt;-cbind(time,TC2)

library(reshape)

mdataTC1 &amp;lt;- melt(TC1,id=c(&amp;quot;time&amp;quot;,&amp;quot;IS&amp;quot;))

mdataTC2 &amp;lt;- melt(TC2,id=c(&amp;quot;time&amp;quot;,&amp;quot;IS&amp;quot;))

#joint the two data
total &amp;lt;- rbind(mdataTC1, mdataTC2)

ggplot(total, aes(fill=IS, y=value, x=factor(time)),variable=X1) + geom_bar(position=&amp;quot;dodge&amp;quot;, stat=&amp;quot;identity&amp;quot;)

#ggplot(total, aes(fill=variable, y=value, x=factor(time)),variable=X1) + geom_bar(position=&amp;quot;fill&amp;quot;, stat=&amp;quot;identity&amp;quot;)

#ggplot(total, aes(y=value, x=variable, color=IS, fill=IS)) + geom_bar( stat=&amp;quot;identity&amp;quot;) +   facet_wrap(~factor(time))

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;

&lt;p&gt;Estimation results are shown in following tables&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;source(&amp;lsquo;C:/Dropbox/workspace/RProjects/PlantAudit/PAG/tc.R&amp;rsquo;)
Replacement of pumps&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]
 [1,] 23791565 20142433 20142433 20142433 20142433 20142433
 [2,] 17680433 21927709 19492677 19492677 19492677 19492677
 [3,] 16295330 16295330 20209870 18863881 18863881 18863881
 [4,] 15018737 15018737 15018737 18626609 18255369 18255369
 [5,] 13842154 13842154 13842154 13842154 17167381 17666486
 [6,] 12757746 12757746 12757746 12757746 12757746 15822471
 [7,] 11758291 11758291 11758291 11758291 11758291 11758291
 [8,] 10837135 10837135 10837135 10837135 10837135 10837135
 [9,]  9988142  9988142  9988142  9988142  9988142  9988142
[10,]  9205661  9205661  9205661  9205661  9205661  9205661
Do Nothing
          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]
 [1,] 20142433 20142433 20142433 20142433 20142433 20142433
 [2,] 19492677 19492677 19492677 19492677 19492677 19492677
 [3,] 18863881 18863881 18863881 18863881 18863881 18863881
 [4,] 18255369 18255369 18255369 18255369 18255369 18255369
 [5,] 17666486 17666486 17666486 17666486 17666486 17666486
 [6,] 17096599 17096599 17096599 17096599 17096599 17096599
 [7,] 16545096 16545096 16545096 16545096 16545096 16545096
 [8,] 16011383 16011383 16011383 16011383 16011383 16011383
 [9,] 15494887 15494887 15494887 15494887 15494887 15494887
[10,] 14995052 14995052 14995052 14995052 14995052 14995052
The difference of investment
          [,1]     [,2]     [,3]       [,4]       [,5]     [,6]
 [1,]  3649131        0        0        0.0        0.0        0
 [2,] -1812244  2435032        0        0.0        0.0        0
 [3,] -2568551 -2568551  1345989        0.0        0.0        0
 [4,] -3236632 -3236632 -3236632   371239.7        0.0        0
 [5,] -3824332 -3824332 -3824332 -3824332.0  -499104.8        0
 [6,] -4338854 -4338854 -4338854 -4338853.7 -4338853.7 -1274128
 [7,] -4786805 -4786805 -4786805 -4786805.2 -4786805.2 -4786805
 [8,] -5174249 -5174249 -5174249 -5174248.9 -5174248.9 -5174249
 [9,] -5506745 -5506745 -5506745 -5506744.7 -5506744.7 -5506745
[10,] -5789391 -5789391 -5789391 -5789390.9 -5789390.9 -5789391
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As can be seen from the table of difference, if pump 1 is to be replaced (TC1) now (year 1), the Owner needs to spend 5 millions Peso for the pump, plus the energy cost for that year, the total cost in year 1 would be 23,791,565 Peso. Meanwhile, if pump 1 is to follow TC2 (Do Nothing), there is a cost of 20,142,433 Peso, which is basically the energy cost. The difference between TC1 and TC2 in year 1 is 3,649,131 Peso, meaning the Owner will spend more money in year 1. In other words, there is a negative cash flow in the first year.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, from 2nd year onward, there is a decreasing in energy consumption, this is thanks to the higher efficiency pump that consume less energy. As a result, there is a saving of -1,812,244 Peso. This value has been discounted and it is the Net Present Value of the OPEX incurred in year two. This difference can be notably seen in the following graph. This graph clearly shows that more energy in following years will be saved if the pump 1 is to be replaced now. It can also be dictated that the amount of cost associated with the saving in energy will compensate the CAPEX within 2 years of investment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ROIpumps/ROIpump_roi.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Excel worksheets combination</title>
      <link>/post/2019-09-15-excel-combine/</link>
      <pubDate>Sun, 15 Sep 2019 23:30:45 +0800</pubDate>
      <guid>/post/2019-09-15-excel-combine/</guid>
      <description>

&lt;p&gt;What if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F.. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.&lt;/p&gt;

&lt;p&gt;What if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F&amp;hellip;. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.&lt;/p&gt;

&lt;p&gt;Let start with time series data that is saved in hundreds of excel worksheets, examples are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Production and consumption data of a single day. Within a day, volumes of production and consumption (e.g. water and energy of a water treatment plant or pump station, number of X and Y produced within a single hours);&lt;/li&gt;
&lt;li&gt;Hydrology data such as rainfall and runoff and other associated parameters
etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ideally, those data should be recorded using relational database structured system such as MySQL or PostgreSQL. However, by default in many organizations, data is recorded in excel file. Everyday, owner of the file just multiply/copy the same worksheet of previous day and repeat the same work. This is OK for him/her but definitely not OK for us, the analyst :).&lt;/p&gt;

&lt;p&gt;This post describes a step by step instruction on how to deal with this issue.&lt;/p&gt;

&lt;h1 id=&#34;examples&#34;&gt;Examples&lt;/h1&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;Example data is with two excel files name data1 and data2. These two files have their structure identical as shown in the followings&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data1
worksheet1
date    from    to  total_pro_hour  total_power_hour    ratio
01-Jan-17   12:00 AM1:00 AM        0.7700000    95   123.38
01-Jan-17   1:00 AM 2:00 AM        0.7400000    86   116.22
01-Jan-17   2:00 AM 3:00 AM        0.6200000    73   117.74
01-Jan-17   3:00 AM 4:00 AM        0.6100000    70   114.75
01-Jan-17   4:00 AM 5:00 AM        0.5700000    62   108.77
01-Jan-17   5:00 AM 6:00 AM        0.7300000    86   117.81
01-Jan-17   6:00 AM 7:00 AM        0.6900000    70   101.45
01-Jan-17   7:00 AM 8:00 AM        1.0100000    93   92.08
01-Jan-17   8:00 AM 9:00 AM        0.9100000    116  127.47
01-Jan-17   9:00 AM 10:00AM        1.0800000    120  111.11
01-Jan-17   10:00 AM 11:00AM       1.0900000    122  111.93
01-Jan-17   11:00 AM 12:00PM       1.0800000    119  110.19
01-Jan-17   12:00 PM 1:00PM        1.1000000    117  106.36
01-Jan-17   1:00 PM 2:00 PM        1.5420000    145  94.03
01-Jan-17   2:00 PM 3:00 PM        0.9330000    136  145.77
01-Jan-17   3:00 PM 4:00 PM        1.0520000    137  130.23
01-Jan-17   4:00 PM 5:00 PM        0.9600000    153  159.38
01-Jan-17   5:00 PM 6:00 PM        0.9910000    135  136.23
01-Jan-17   6:00 PM 7:00 PM        1.0110000    146  144.41
01-Jan-17   7:00 PM 8:00 PM        0.9320000    134  143.78
01-Jan-17   8:00 PM 9:00 PM        0.9680000    133  137.40
01-Jan-17   9:00 PM 10:00 PM       0.7110000    124  174.40
01-Jan-17   10:00 PM 11:00 PM   0.7800000   72   92.31
01-Jan-17   11:00 PM 12:00 AM   0.6100000   75   122.95

worksheet 2
02-Jan-17   12:00 AM 1:00 AM    0.5600000   48   85.71
02-Jan-17   1:00 AM  2:00 AM    0.4420000   42   95.02
02-Jan-17   2:00 AM  3:00 AM    0.4700000   40   85.11
02-Jan-17   3:00 AM  4:00 AM    0.6980000   59   84.53
02-Jan-17   4:00 AM  5:00 AM    0.8200000   86   104.88
02-Jan-17   5:00 AM  6:00 AM    0.4700000   48   102.13
02-Jan-17   6:00 AM  7:00 AM    1.0400000   121  116.35
02-Jan-17   7:00 AM  8:00 AM    1.0800000   146  135.19
02-Jan-17   8:00 AM  9:00 AM    1.0800000   122  112.96
02-Jan-17   9:00 AM  10:00 AM   0.9600000   82   85.42
02-Jan-17   10:00 AM 11:00 AM   0.9100000   73   80.22
02-Jan-17   11:00 AM 12:00 PM   0.8500000   65   76.47
02-Jan-17   12:00 PM 1:00 PM    0.7100000   57   80.28
02-Jan-17   1:00 PM  2:00 PM    0.9690000   48   49.54
02-Jan-17   2:00 PM  3:00 PM    0.8310000   65   78.22
02-Jan-17   3:00 PM  4:00 PM    1.1290000   96   85.03
02-Jan-17   4:00 PM  5:00 PM    1.2300000   109  88.62
02-Jan-17   5:00 PM  6:00 PM    1.1210000   114  101.69
02-Jan-17   6:00 PM  7:00 PM    0.9440000   110  116.53
02-Jan-17   7:00 PM  8:00 PM    0.9790000   112  114.40
02-Jan-17   8:00 PM  9:00 PM    1.0260000   108  105.26
02-Jan-17   9:00 PM 10:00 PM    0.8710000   118  135.48
02-Jan-17   10:00 PM 11:00 PM   0.8100000   99   122.22
02-Jan-17   11:00 PM 12:00 AM   0.6800000   79   116.18
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This excel file contains 2 worksheet “1” and “2” that record hourly production (ML) and energy consumption (KW) of a water pump station.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;column is the date;&lt;/li&gt;
&lt;li&gt;column 2 and 3 are hours;&lt;/li&gt;
&lt;li&gt;column 4 is production data in million liter (ML);&lt;/li&gt;
&lt;li&gt;column 5 is energy consumption data in KW&lt;/li&gt;
&lt;li&gt;column 6 is ratio between energy consumption and production, basically it is the division of column 5 and 4.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This kind of data is recorded hourly and there will be about 30 worksheets for one month. Let say you have 5 or 10 years production data saving in excel files like this and you need to combine all of them into a single frame for Business Analysis purpose. It will be a nightmare if you just copy and paste 🙂 terrible excel.&lt;/p&gt;

&lt;h2 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;It is assumed that all worksheets in all excel files are identical in their structure. This is not a realistic assumption as excel users cannot be consistent with their data. Data in their excel files are&lt;/p&gt;

&lt;p&gt;Mixed up with numeric and text even for the same attributes;
Merging cells, adding new rows and columns that make them homogeneous.
Solving such problem is not the objective of this post, but it is worth to mention that before we get a good set of data, we probably need to do some Coding in Visual Basic to standardize the excel worksheets, or we need to do a certain level of manual data compiling before we can run the code in R.&lt;/p&gt;

&lt;p&gt;Will cover how to standardize using the same sets of data in other post.&lt;/p&gt;

&lt;h2 id=&#34;combining-worksheets-using-navicat-and-mysql&#34;&gt;Combining worksheets using Navicat and MySQL&lt;/h2&gt;

&lt;p&gt;Why &lt;a href=&#34;https://www.navicat.com/en/&#34; target=&#34;_blank&#34;&gt;NAVICAT&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;–&amp;gt; Navicat offers a handy way to import data from excel files. It can import multiple worksheets in one single click into respective tables of MySQL. I find this feature superior than other open source SQL Client such PhPmyAdmin, Dbeaver, etc.&lt;/p&gt;

&lt;p&gt;Herein, I demonstrate some steps to import the example data files.&lt;/p&gt;

&lt;p&gt;a. Create table&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create
table
`1` (
date varchar(255),
`from` varchar(255),
`to` varchar(255),
total_pro_hour varchar(255),
total_power_hour varchar(255),
ratio varchar(255)
);

create table `2` as select * from `1`;
create table `3` as select * from `1`;
create table `4` as select * from `1`;
create table `5` as select * from `1`;
create table `6` as select * from `1`;
create table `7` as select * from `1`;
create table `8` as select * from `1`;
create table `9` as select * from `1`;
create table `10` as select * from `1`;
create table `11` as select * from `1`;
create table `12` as select * from `1`;
create table `13` as select * from `1`;
create table `14` as select * from `1`;
create table `15` as select * from `1`;
create table `16` as select * from `1`;
create table `17` as select * from `1`;
create table `18` as select * from `1`;
create table `19` as select * from `1`;
create table `20` as select * from `1`;
create table `21` as select * from `1`;
create table `22` as select * from `1`;
create table `23` as select * from `1`;
create table `24` as select * from `1`;
create table `25` as select * from `1`;
create table `26` as select * from `1`;
create table `27` as select * from `1`;
create table `28` as select * from `1`;
create table `29` as select * from `1`;
create table `30` as select * from `1`;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This SQL creates 30 tables name from 1 to 30 that correspond to each day in a month. Note that for February, there are 28 days but dont worry, we still use 30 or 31 worksheets as additional worksheets will be blank anyway and make no harm to the operation.&lt;/p&gt;

&lt;p&gt;b. Manual importing worksheets from the excel file to MySQL table using NaviCAT&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Select table 1 in MySQL database&lt;/li&gt;
&lt;li&gt;Right Click –&amp;gt; Import Wizard&lt;/li&gt;
&lt;li&gt;Select Excel file and Click Next&lt;/li&gt;
&lt;li&gt;Import the excel file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/navicat_importexcel.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/navicat_importtablemapping.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/navicat_importattributemapping.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/navicat_importappending.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/navicat_importstart.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/navicat_tableraw.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now you have all tables you need in MySQL. However, Values of time shown in FROM and TO column have change from AM, PM to something else. For example 1900-01-02 should be 24. To solve this issue, we will use the following SQL syntax&lt;/p&gt;

&lt;p&gt;c. Rename tables&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;RENAME TABLE
`1` TO pat_2018_10_1,
`2` TO pat_2018_10_2,
`3` TO pat_2018_10_3,
`4` TO pat_2018_10_4,
`5` TO pat_2018_10_5,
`6` TO pat_2018_10_6,
`7` TO pat_2018_10_7,
`8` TO pat_2018_10_8,
`9` TO pat_2018_10_9,
`10` TO pat_2018_10_10,
`11` TO pat_2018_10_11,
`12` TO pat_2018_10_12,
`13` TO pat_2018_10_13,
`14` TO pat_2018_10_14,
`15` TO pat_2018_10_15,
`16` TO pat_2018_10_16,
`17` TO pat_2018_10_17,
`18` TO pat_2018_10_18,
`19` TO pat_2018_10_19,
`20` TO pat_2018_10_20,
`21` TO pat_2018_10_21,
`22` TO pat_2018_10_22,
`23` TO pat_2018_10_23,
`24` TO pat_2018_10_24,
`25` TO pat_2018_10_25,
`26` TO pat_2018_10_26,
`27` TO pat_2018_10_27,
`28` TO pat_2018_10_28,
`29` TO pat_2018_10_29,
`30` TO pat_2018_10_30
;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we rename the table to whatever we need. By doing so, we can resue the Create Query to perform the same procedure for importing new table.&lt;/p&gt;

&lt;p&gt;d. Create Production table&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create
table
production (
date varchar(255),
`from` varchar(255),
`to` varchar(255),
total_pro_hour varchar(255),
total_power_hour varchar(255),
ratio varchar(255)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code creates a production table.&lt;/p&gt;

&lt;p&gt;e. Import/Append all raw tables into one table – Production&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;INSERT INTO production
SELECT *
FROM pat_2017_1_1;

INSERT INTO production
SELECT *
FROM pat_2017_1_2;

INSERT INTO production
SELECT *
FROM pat_2017_1_3;

INSERT INTO production
SELECT *
FROM pat_2017_1_4;

INSERT INTO production
SELECT *
FROM pat_2017_1_5;
INSERT INTO production
SELECT *
FROM pat_2017_1_6;
INSERT INTO production
SELECT *
FROM pat_2017_1_7;
INSERT INTO production
SELECT *
FROM pat_2017_1_8;
INSERT INTO production
SELECT *
FROM pat_2017_1_9;
INSERT INTO production
SELECT *
FROM pat_2017_1_10;
INSERT INTO production
SELECT *
FROM pat_2017_1_11;
INSERT INTO production
SELECT *
FROM pat_2017_1_12;
INSERT INTO production
SELECT *
FROM pat_2017_1_13;
INSERT INTO production
SELECT *
FROM pat_2017_1_14;
INSERT INTO production
SELECT *
FROM pat_2017_1_15;
INSERT INTO production
SELECT *
FROM pat_2017_1_16;
INSERT INTO production
SELECT *
FROM pat_2017_1_17;
INSERT INTO production
SELECT *
FROM pat_2017_1_18;
INSERT INTO production
SELECT *
FROM pat_2017_1_19;
INSERT INTO production
SELECT *
FROM pat_2017_1_20;
INSERT INTO production
SELECT *
FROM pat_2017_1_21;
INSERT INTO production
SELECT *
FROM pat_2017_1_22;
INSERT INTO production
SELECT *
FROM pat_2017_1_23;
INSERT INTO production
SELECT *
FROM pat_2017_1_24;
INSERT INTO production
SELECT *
FROM pat_2017_1_25;
INSERT INTO production
SELECT *
FROM pat_2017_1_26;
INSERT INTO production
SELECT *
FROM pat_2017_1_27;
INSERT INTO production
SELECT *
FROM pat_2017_1_28;
INSERT INTO production
SELECT *
FROM pat_2017_1_29;
INSERT INTO production
SELECT *
FROM pat_2017_1_30;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code imports all data from raw table into production table.&lt;/p&gt;

&lt;p&gt;f. Correct data, particularly with date and time&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE IF EXISTS productioncorrected;

create table productioncorrected select * from production;


UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;1900-01-01&#39;,&#39;24:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;1900-01-02 &#39;,&#39;&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;1900-01-02&#39;,&#39;00:00:00.000&#39;);

#####
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;1:00AM&#39;,&#39;01:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;2:00AM&#39;,&#39;02:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;3:00AM&#39;,&#39;03:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;4:00AM&#39;,&#39;04:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;5:00AM&#39;,&#39;05:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;6:00AM&#39;,&#39;06:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;7:00AM&#39;,&#39;07:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;8:00AM&#39;,&#39;08:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;9:00AM&#39;,&#39;09:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;10:00AM&#39;,&#39;10:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;11:00AM&#39;,&#39;11:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;12:00PM&#39;,&#39;12:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;01:00PM&#39;,&#39;13:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;02:00PM&#39;,&#39;14:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;03:00PM&#39;,&#39;15:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;04:00PM&#39;,&#39;16:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;05:00PM&#39;,&#39;17:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;06:00PM&#39;,&#39;18:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;07:00PM&#39;,&#39;19:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;08:00PM&#39;,&#39;20:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;09:00PM&#39;,&#39;21:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;10:00PM&#39;,&#39;22:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;11:00PM&#39;,&#39;23:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;12:00AM&#39;,&#39;00:00:00.000&#39;);


#####
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;1:00AM&#39;,&#39;01:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;2:00AM&#39;,&#39;02:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;3:00AM&#39;,&#39;03:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;4:00AM&#39;,&#39;04:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;5:00AM&#39;,&#39;05:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;6:00AM&#39;,&#39;06:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;7:00AM&#39;,&#39;07:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;8:00AM&#39;,&#39;08:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;9:00AM&#39;,&#39;09:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;10:00AM&#39;,&#39;10:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;11:00AM&#39;,&#39;11:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;12:00PM&#39;,&#39;12:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;01:00PM&#39;,&#39;13:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;02:00PM&#39;,&#39;14:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;03:00PM&#39;,&#39;15:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;04:00PM&#39;,&#39;16:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;05:00PM&#39;,&#39;17:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;06:00PM&#39;,&#39;18:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;07:00PM&#39;,&#39;19:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;08:00PM&#39;,&#39;20:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;09:00PM&#39;,&#39;21:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;10:00PM&#39;,&#39;22:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;11:00PM&#39;,&#39;23:00:00.000&#39;);
UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;12:00AM&#39;,&#39;24:00:00.000&#39;);

UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;    101:00:00.000&#39;,&#39;11:00:00.000&#39;);

UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,&#39;    102:00:00.000&#39;,&#39;24:00:00.000&#39;);

UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;    101:00:00.000&#39;,&#39;11:00:00.000&#39;);

UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,&#39;    102:00:00.000&#39;,&#39;24:00:00.000&#39;);


UPDATE productioncorrected SET productioncorrected.`to` = &amp;quot;24:00:00.000&amp;quot;
WHERE productioncorrected.`from`= &amp;quot;23:00:00.000&amp;quot;;


UPDATE productioncorrected SET productioncorrected.`to` = &amp;quot;11:00:00.000&amp;quot;
WHERE productioncorrected.`from`= &amp;quot;10:00:00.000&amp;quot;;

UPDATE productioncorrected SET productioncorrected.`from` = &amp;quot;11:00:00.000&amp;quot;
WHERE productioncorrected.`to`= &amp;quot;12:00:00.000&amp;quot;;

UPDATE productioncorrected SET productioncorrected.`from` = &amp;quot;24:00:00.000&amp;quot;
WHERE productioncorrected.`to`= &amp;quot;01:00:00.000&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code is used to correct&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;time data&lt;/li&gt;
&lt;li&gt;missing data&lt;/li&gt;
&lt;li&gt;outliner&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;g. Create analysis table&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE IF EXISTS analysis;

CREATE TABLE analysis
Select *

from
productioncorrected
where productioncorrected.total_pro_hour &amp;gt;0
and
productioncorrected.total_power_hour&amp;gt;0
and
productioncorrected.total_power_hour NOT LIKE &amp;quot;-&amp;quot;
;

ALTER TABLE analysis MODIFY COLUMN total_power_hour DOUBLE;

ALTER TABLE analysis MODIFY COLUMN total_pro_hour DOUBLE;

ALTER TABLE analysis MODIFY COLUMN ratio DOUBLE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Analysis table is basically the same with production but with some correction&lt;/p&gt;

&lt;p&gt;h. Create Analysis Date table&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE IF EXISTS analysisdate;

CREATE TABLE analysisdate
Select
*
from
analysis
GROUP BY analysis.date;

ALTER TABLE analysisdate
ADD id int NOT NULL AUTO_INCREMENT PRIMARY KEY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This SQL code will create Analysis Date table that basically summary hourly data into daily data.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;WHAO, after going through each of the above Query, my figures are tired already. You can further enhance SQL code to make the entire process faster, but still it is much slower than using below R code.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;combining-worksheets-using-r&#34;&gt;Combining worksheets using R&lt;/h1&gt;

&lt;p&gt;Below R codes are extracted from &lt;a href=&#34;https://github.com/namkyodai/BusinessAnalytics/tree/master/ExcelSheetCombine&#34; target=&#34;_blank&#34;&gt;Github source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This code is used to combine multiple excel worksheets into one dataframe. Particularly useful when combining multiple worksheets of production data with each worksheet is a date of a month, and in each worksheet, data is saved in hourly basis.&lt;/p&gt;

&lt;p&gt;the first method, using XlConnect.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# this method has a limitation that XlConnect doesnt work well with excel files with dynamic links. When importing into R, it gives NA values.
# to make sure that importing is perfect. It is advisable to disable all dynamic links in excel file by going to Data
library(XLConnect)
# load data file (excel files ended with cls, xlsc, or xlsm)
datafile &amp;lt;- loadWorkbook(&amp;quot;data1.xlsx&amp;quot;) # This is a static worksheet, without any dynamic links
# obtain sheet names
worksheets &amp;lt;- getSheets(datafile)
names(worksheets) &amp;lt;- worksheets
# dataframe
worksheets_list &amp;lt;- lapply(worksheets, function(.sheet){readWorksheet(object=datafile, .sheet)})
# limit worksheet_list to sheets with at least 1 dimension
worksheets_list2 &amp;lt;- worksheets_list[sapply(worksheets_list, function(x) dim(x)[1]) &amp;amp;gt; 0]

# code to read in each excel worksheet as individual dataframes
 for (i in 2:length(worksheets_list2)){assign(paste0(&amp;quot;df&amp;quot;, i), as.data.frame(worksheets_list2[i]))}

# define function to clean data in each data frame (updated based on your data). You must define here carefully otherwise it will not work well with some certain type of data. The fastest way is only drop out missing values. Other value can be dealed with using query in MySQL
cleaner &amp;lt;- function(df){
  # drop rows with missing values
  df &amp;lt;- df[rowSums(is.na(df)) == 0,]
  # remove serial comma from all variables
 # df[,-1] &amp;lt;- as.numeric(gsub(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;, as.matrix(df[,-1])))
  # create numeric version of year variable for graphing
 # df$Year &amp;lt;- as.numeric(substr(df$year, 1, 4))
# return cleaned df
  return(df)
}

# clean sheets and create one data frame
data1 &amp;lt;- do.call(rbind,lapply(names(worksheets_list2), function(x) cleaner(worksheets_list2[[x]])))

cat(&amp;quot;Print out the data 1 frame \n&amp;quot;)
print(data1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Method is with readxl package. This is superior than the former one as readxl can handle excel files with dynamic links. This means it will retain values and ignore the links.
# ----------------
library(readxl)
read_excel_allsheets &amp;lt;- function(filename, tibble = FALSE) {
  # I prefer straight data.frames
  # but if you like tidyverse tibbles (the default with read_excel)
  # then just pass tibble = TRUE
  sheets &amp;lt;- readxl::excel_sheets(filename)
  x &amp;lt;- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
  if(!tibble) x &amp;lt;- lapply(x, as.data.frame)
  names(x) &amp;lt;- sheets
  x
}

#start to read and write data into csv file
worksheets &amp;lt;- read_excel_allsheets(&amp;quot;data1.xlsx&amp;quot;)
source(&amp;quot;cleaning.R&amp;quot;)
filedata &amp;lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))
write.table(filedata, &amp;quot;myDF.csv&amp;quot;, sep = &amp;quot;,&amp;quot;, col.names = !file.exists(&amp;quot;myDF.csv&amp;quot;), row.names=FALSE, append = T)

worksheets &amp;lt;- read_excel_allsheets(&amp;quot;data2.xlsm&amp;quot;)
source(&amp;quot;cleaning.R&amp;quot;)
filedata &amp;lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))
write.table(filedata, &amp;quot;myDF.csv&amp;quot;, sep = &amp;quot;,&amp;quot;, col.names = !file.exists(&amp;quot;myDF.csv&amp;quot;), row.names=FALSE, append = T)
#end
#https://medium.com/@niharika.goel/merge-multiple-csv-excel-files-in-a-folder-using-r-e385d962a90a
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;combining-multiple-excel-files-using-r&#34;&gt;Combining multiple excel files using R&lt;/h1&gt;

&lt;p&gt;I found the code from Niharika suits the purpose of this exercise. Kindly refer to her github site for original code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/NiharikaGoel12/R-Playground&#34; target=&#34;_blank&#34;&gt;https://github.com/NiharikaGoel12/R-Playground&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Her readme file states
This repository contains basic codes for R, which might be useful in day to day work, especially doing data analysis on large datasets in Excel or CSV.&lt;/p&gt;

&lt;p&gt;#Merge multiple Excel/CSV files in a folder&lt;/p&gt;

&lt;p&gt;Consider a case when you have multiple xlsx/csv files in a folder &amp;amp; you to merge them into one single file. Here, I have used lapply() which returns a list of the same length as i. And grepl() will check exact match between merge_file_name &amp;amp; existing file ‘i’. In this case, if the two files are same, we will ignore already created “merge file”.&lt;/p&gt;

&lt;p&gt;rbind() will combine data frame by rows and merge all the files.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;a. Combining CSV files&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;path &amp;lt;- &amp;quot;sample-data/merge-files/csv&amp;quot;
merge_file_name &amp;lt;- &amp;quot;sample-data/merge-files/merged_file.csv&amp;quot;

filenames &amp;lt;- list.files(path= path, full.names=TRUE)

All &amp;lt;- lapply(filenames,function(filename){
    print(paste(&amp;quot;Merging&amp;quot;,filename,sep = &amp;quot; &amp;quot;))
    read.csv(filename)
})
df &amp;lt;- do.call(rbind.data.frame, All)
write.csv(df,merge_file_name)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b. Combining excel files&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(openxlsx)

path &amp;lt;- &amp;quot;sample-data/merge-files/xlsx&amp;quot;
merge_file_name &amp;lt;- &amp;quot;sample-data/merge-files/merged_file.xlsx&amp;quot;

filenames_list &amp;lt;- list.files(path= path, full.names=TRUE)

All &amp;lt;- lapply(filenames_list,function(filename){
    print(paste(&amp;quot;Merging&amp;quot;,filename,sep = &amp;quot; &amp;quot;))
    read.xlsx(filename)
})

df &amp;lt;- do.call(rbind.data.frame, All)
write.xlsx(df,merge_file_name)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c. Combining the example data&lt;/p&gt;

&lt;p&gt;We can use the code in step a and b of this section. However, there is one draw back that we need to save our data first into csv or excel file. This is also a bit of time consuming. To avoid this, we can just simply write directly data into csv file as presented in the last part of section 4.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# ----------------
library(readxl)
read_excel_allsheets &amp;lt;- function(filename, tibble = FALSE) {
  # I prefer straight data.frames
  # but if you like tidyverse tibbles (the default with read_excel)
  # then just pass tibble = TRUE
  sheets &amp;lt;- readxl::excel_sheets(filename)
  x &amp;lt;- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
  if(!tibble) x &amp;lt;- lapply(x, as.data.frame)
  names(x) &amp;lt;- sheets
  x
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#start to read and write data into csv file
worksheets &amp;lt;- read_excel_allsheets(&amp;quot;data1.xlsx&amp;quot;)
source(&amp;quot;cleaning.R&amp;quot;)
filedata &amp;lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))
write.table(filedata, &amp;quot;myDF.csv&amp;quot;, sep = &amp;quot;,&amp;quot;, col.names = !file.exists(&amp;quot;myDF.csv&amp;quot;), row.names=FALSE, append = T)


worksheets &amp;lt;- read_excel_allsheets(&amp;quot;data2.xlsm&amp;quot;)
source(&amp;quot;cleaning.R&amp;quot;)
filedata &amp;lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]])))
write.table(filedata, &amp;quot;myDF.csv&amp;quot;, sep = &amp;quot;,&amp;quot;, col.names = !file.exists(&amp;quot;myDF.csv&amp;quot;), row.names=FALSE, append = T)

#end

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code read 2 data files data1.xlsx and data2.xlsm, combining all worksheets of these two files and then write the data frame to a CSV file named myDF.csv. If we have more than 2 files to read and combine, we can just copy the code and paste it under and remember to change the source data file. We can also automate this process by making a loop, which will be presented in other post.&lt;/p&gt;

&lt;p&gt;Once we have the combined CSV file, we can use NaviCAT to import this file to MySQL for further enhancement as presented in section 3. In the end, we can summary the production and energy consumption data in day or week that will be useful for visualization, correlation, and regression analysis.&lt;/p&gt;

&lt;h1 id=&#34;visualization&#34;&gt;Visualization&lt;/h1&gt;

&lt;h2 id=&#34;tableu&#34;&gt;Tableu&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/fig_energy_3monthbarchart.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/fig_energy_3monthtrend.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(DBI)
library(RODBC)
library(RMySQL)
library(xts)
library(ggplot2)
library(hydroTSM) #call the hydrology package for time series analysis
#this code is used for energy audit of the plant based on production and power consumption
dataproduction = dbConnect(MySQL(), user=&#39;root&#39;, password=&#39;&#39;, dbname=&#39;exampledata&#39;, host=&#39;localhost&#39;)
#exampledata is the database stored in your MySQL server
dbListTables(dataproduction)
dbListFields(dataproduction, &#39;analysisdate&#39;)
rs = dbSendQuery(dataproduction, &amp;quot;select * from analysisdate&amp;quot;)
rs=dbFetch(rs, n = -1)

data=c(rs[1],rs[4],rs[5],rs[6])
a=as.Date(data$date,&amp;quot;%Y-%m-%d&amp;quot;)
production=data$total_pro_hour
power=data$total_power_hour
ratio=data$ratio
data=data.frame(a,production,power,ratio)
#Correlation analysis
hydropairs(data[,2:3])
#Plot production data
plot.new()
p=ggplot(data=data,aes(a, production))+ geom_line(color = &amp;quot;#00AFBB&amp;quot;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12))
p
min &amp;lt;- as.Date(&amp;quot;2017-01-01&amp;quot;)
max &amp;lt;-  as.Date(&amp;quot;2018-10-30&amp;quot;)
#p + scale_x_date(limits = c(min, max))
pp=p+scale_x_date(date_labels = &amp;quot;%Y&amp;quot;,date_breaks=&amp;quot;year&amp;quot;,limits = c(min, max))+labs(title = &amp;quot;Production hourly&amp;quot;,x=&amp;quot;Years&amp;quot;,y=&amp;quot;ML&amp;quot;)+scale_y_continuous(limits=c(0,2))#+ stat_smooth(method=&amp;quot;lm&amp;quot;)
ggsave(&amp;quot;ch05_fig_energy_production.png&amp;quot;, plot = pp)

print(pp)
#Plot power consumption data
plot.new()
q=ggplot(data=data,aes(a, power))+ geom_line(color = &amp;quot;#00AFBB&amp;quot;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12))
#q
min &amp;lt;- as.Date(&amp;quot;2017-01-01&amp;quot;)
max &amp;lt;-  as.Date(&amp;quot;2018-10-30&amp;quot;)
#p + scale_x_date(limits = c(min, max))
qq=q+scale_x_date(date_labels = &amp;quot;%Y&amp;quot;,date_breaks=&amp;quot;year&amp;quot;,limits = c(min, max))+labs(title = &amp;quot;Power hourly&amp;quot;,x=&amp;quot;Years&amp;quot;,y=&amp;quot;KW&amp;quot;)+scale_y_continuous(limits=c(0,250))#+ stat_smooth(method=&amp;quot;lm&amp;quot;)
ggsave(&amp;quot;ch05_fig_energy_power.png&amp;quot;, plot = qq)
print(qq)
#plot ratio
plot.new()
w=ggplot(data=data,aes(a, ratio))+ geom_line(color = &amp;quot;#00AFBB&amp;quot;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12))
#w
min &amp;lt;- as.Date(&amp;quot;2017-01-01&amp;quot;)
max &amp;lt;-  as.Date(&amp;quot;2018-10-30&amp;quot;)
#p + scale_x_date(limits = c(min, max))
ww=w+scale_x_date(date_labels = &amp;quot;%Y&amp;quot;,date_breaks=&amp;quot;year&amp;quot;,limits = c(min, max))+labs(title = &amp;quot;Ratio&amp;quot;,x=&amp;quot;Years&amp;quot;,y=&amp;quot;&amp;quot;)+scale_y_continuous(limits=c(0,250))#+ stat_smooth(method=&amp;quot;lm&amp;quot;)
ggsave(&amp;quot;ch05_fig_energy_ratio.png&amp;quot;, plot = ww)
print(ww)
dbDisconnect(dataproduction)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/ch05_fig_energy_correlation.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/ch05_fig_energy_power.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/ch05_fig_energy_production.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://namkyodai.github.io&#34; rel=&#34;some text&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/ExcelSheetCombine/ch05_fig_energy_ratio.png&#34; alt=&#34;Foo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>900MW Power Station - KEPCO E&amp;C</title>
      <link>/project/2016-kepco/</link>
      <pubDate>Tue, 19 Apr 2016 11:08:22 +0800</pubDate>
      <guid>/project/2016-kepco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Infrastructure Deterioration Prediction with a Poisson Hidden Markov Model on Time Series Data  </title>
      <link>/publication/lethanh2015b/</link>
      <pubDate>Fri, 19 Sep 2014 13:20:32 +0800</pubDate>
      <guid>/publication/lethanh2015b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A statistical deterioration forecasting method using hidden Markov model for infrastructure management</title>
      <link>/publication/kobayashi2012a/</link>
      <pubDate>Sat, 19 May 2012 13:20:32 +0800</pubDate>
      <guid>/publication/kobayashi2012a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Bayesian Estimation Method to Improve Deterioration Prediction for Infrastructure System with Markov Chain Model</title>
      <link>/publication/kobayashi2012/</link>
      <pubDate>Mon, 19 Mar 2012 13:20:32 +0800</pubDate>
      <guid>/publication/kobayashi2012/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
