[{"authors":["admin"],"categories":null,"content":"Đam mê chạy bộ, lặn biển, và lập trình web và xác xuất thống kê. Hiện tại sống tại Manila, Philippines và đã từng sống tại Băng Cốc, Kyoto, Osaka, Zurich, Geneva, and Hà nội.\nNơi coi là nhà: Ecopark (VN), Katsura (JP), Geneva (CH), and Zurich (CH)\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"vi","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/vi/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vi/authors/admin/","section":"authors","summary":"Đam mê chạy bộ, lặn biển, và lập trình web và xác xuất thống kê. Hiện tại sống tại Manila, Philippines và đã từng sống tại Băng Cốc, Kyoto, Osaka, Zurich, Geneva, and Hà nội.\nNơi coi là nhà: Ecopark (VN), Katsura (JP), Geneva (CH), and Zurich (CH)","tags":null,"title":"Lê Thanh Nam","type":"authors"},{"authors":[],"categories":["Project Management","Plant Audit","Asset Management"],"content":"  Client: Maynilad Water Services Inc. Name: Rehabilitation, Retrofitting and Process Improvement of Lamesa 2 Water Treatment Plant Theme: Asset Management, Process Improvement, Retrofitting, Rehabilitation Scale: 900 MLD Timeline: Engineering (Jun/2016-Dec/2016), Procurement and Bidding (Aug/2016-Sep/2017), Construction (Oct/2017 onward)  Objectives  Upgrade the existing Pulsator WTP (900MLD) to cope with  High turbidity events in raining seasons (e.g. NTU up to 2000); Slow performance of Sludge Extraction System under high turbidity events; Rapidlly deterioration and corrosion of structures and assets; 7.2 magnitute of earthquake.  Provide better integration and connectivity with the Lamesa 1 Water Treatment Plant (1200 MLD) and the Sludge Treatment Facility (STF) being designed and constructed by JFE Contractor. Provide fully Automation and integration (e.g. SCADA)  Roles and Responsibilities  Project Management  Lesson Learned Authorship Back to Projects\n","date":1571454502,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1571454502,"objectID":"d3985886bb985f35c849fd60437363a1","permalink":"/vi/project/2016-lamesa2/","publishdate":"2019-10-19T11:08:22+08:00","relpermalink":"/vi/project/2016-lamesa2/","section":"project","summary":"Rehabilitation, Retrofitting, and Process Improvement of Lamesa 2 Water Treatment Plant","tags":["Project Management","Plant Audit","Asset Management","Water"],"title":"Lamesa 2 - Maynilad","type":"project"},{"authors":[],"categories":["Project Management","Plant Audit","Asset Management"],"content":" The Project  Client: Maynilad Water Services Inc. Name: Plant Audit of Various Pump Stations and Reservoirs Theme: Asset Management  Objectives  Visual Inspection and Testing on mechanical assets (e.g. pump, piping, valves) and electrical assets (e.g. GENSETs, transformers, MCC, Motors) using developed set condition states and standards; Energy Audit; FDAS and WEM studies; Determination of optimal 5 years intervention program; Life cycle cost analysis and estimation of Return on Investment.  Responsibilities  Record inspection and testing data in a centralized relational database system (MySQL); Big data analysis and data solutions on 5-10 years operation data to detect trend and reliability of the system; Investigation of current design and operational scheme using qualitative reliability analysis (e.g. FMEA); Weibull Analysis and Faul Tree Analysis to estimate reliability; Development of a Life Cycle Cost model using reliability concept to determine optimal intervention strategies and program; Organizing a series of training and technical workshops on reliability study and modelling used in the project.  Lesson Learned Authorship  Nam led the team and trained the team to use LaTEX system to compile all reports  Back to Projects\n","date":1571454502,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1571454502,"objectID":"44d0d05e3debf4fb462bb33a5a870324","permalink":"/vi/project/2018-plantaudit/","publishdate":"2019-10-19T11:08:22+08:00","relpermalink":"/vi/project/2018-plantaudit/","section":"project","summary":"Plant Audit and Asset Management for Various Pump Stations and Reservoirs","tags":["Project Management","Plant Audit","Asset Management","Water"],"title":"Plant Audit - Maynilad","type":"project"},{"authors":null,"categories":null,"content":" This example shows a very simple regression analysis using fuel data of 38 cars. Code is copied from the book “Data Mining and Business Analytics with R” with minor modification on the graphs.\nThere are 2 learning points to be remembered with this example.\nLeaps package with regsubsets fuction. Regsubsets function allows to perform regression on subsets of data, particularly useful to compare across model’s predictors to understand the fit of the model.\nhttps://www.rdocumentation.org/packages/leaps/versions/2.1-1/topics/regsubsets\nCross-Validation  Cross-validation removes one case from the data set of n cases, fits the model to the reduced data set, and predicts the response of that one case that has been removed from the estimation. This is repeated for each of the n cases. The summary statistics of the n genuine out-of-sample prediction errors (mean error, root mean square error, mean absolute percent error) help us assess the out-of-sample prediction performance. Cross-validation is very informative as it evaluates the model on new data. We find that the model with all six regressors performs better. It leads to a mean absolute percent error of about 6.75% (as compared to 8.23% for the model with weight as the only regressor).\n \u0026gt; FuelEff GPM WT DIS NC HP ACC ET 1 5.917 4.360 350 8 155 14.9 1 2 6.452 4.054 351 8 142 14.3 1 3 5.208 3.605 267 8 125 15.0 1 4 5.405 3.940 360 8 150 13.0 1 5 3.333 2.155 98 4 68 16.5 0 6 3.636 2.560 134 4 95 14.2 0 7 3.676 2.300 119 4 97 14.7 0 8 3.236 2.230 105 4 75 14.5 0 9 4.926 2.830 131 5 103 15.9 0 10 5.882 3.140 163 6 125 13.6 0 11 4.630 2.795 121 4 115 15.7 0 12 6.173 3.410 163 6 133 15.8 0 13 4.854 3.380 231 6 105 15.8 0 14 4.808 3.070 200 6 85 16.7 0 15 5.376 3.620 225 6 110 18.7 0 16 5.525 3.410 258 6 120 15.1 0 17 5.882 3.840 305 8 130 15.4 1 18 5.682 3.725 302 8 129 13.4 1 19 6.061 3.955 351 8 138 13.2 1 20 5.495 3.830 318 8 135 15.2 1 21 3.774 2.585 140 4 88 14.4 0 22 4.566 2.910 171 6 109 16.6 1 23 2.933 1.975 86 4 65 15.2 0 24 2.849 1.915 98 4 80 14.4 0 25 3.650 2.670 121 4 80 15.0 0 26 3.175 1.990 89 4 71 14.9 0 27 3.390 2.135 98 4 68 16.6 0 28 3.521 2.670 151 4 90 16.0 0 29 3.472 2.595 173 6 115 11.3 1 30 3.731 2.700 173 6 115 12.9 1 31 2.985 2.556 151 4 90 13.2 0 32 2.924 2.200 105 4 70 13.2 0 33 3.145 2.020 85 4 65 19.2 0 34 2.681 2.130 91 4 69 14.7 0 35 3.279 2.190 97 4 78 14.1 0 36 4.545 2.815 146 6 97 14.5 0 37 4.651 2.600 121 4 110 12.8 0 38 3.135 1.925 89 4 71 14.0 0  CODE ## first we read in the data FuelEff \u0026lt;- read.csv(\u0026quot;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/FuelEfficiency.csv\u0026quot;) #FuelEff \u0026lt;- read.csv(\u0026quot;FuelEfficiency.csv\u0026quot;) FuelEff #---------------------------- plot.new() #par(mfrow=c(1,1)) par(mar=c(4,4,1,1)+0.1,mfrow=c(3,3),bg=\u0026quot;white\u0026quot;,cex = 1, cex.main = 1) plot(GPM~MPG,data=FuelEff) plot(GPM~WT,data=FuelEff) plot(GPM~DIS,data=FuelEff) plot(GPM~NC,data=FuelEff) plot(GPM~HP,data=FuelEff) plot(GPM~ACC,data=FuelEff) plot(GPM~ET,data=FuelEff) dev.copy(png,'fueleff_xyplot.png',width = 800, height = 800) dev.off() FuelEff=FuelEff[-1] #ignore the MPG column FuelEff ## regression on all data m1=lm(GPM~.,data=FuelEff) summary(m1) cor(FuelEff) ## best subset regression in R library(leaps) X=FuelEff[,2:7] y=FuelEff[,1] #use the regsubsets function from package leaps to compute regression of the subsets #https://www.rdocumentation.org/packages/leaps/versions/2.1-1/topics/regsubsets out=summary(regsubsets(X,y,nbest=2,nvmax=ncol(X))) tab=cbind(out$which,out$rsq,out$adjr2,out$cp) tab m2=lm(GPM~WT,data=FuelEff) summary(m2) ## cross-validation (leave one out) for the model on all six regressors n=length(FuelEff$GPM) diff=dim(n) percdiff=dim(n) for (k in 1:n) { train1=c(1:n) train=train1[train1!=k] ## the R expression \u0026quot;train1[train1!=k]\u0026quot; picks from train1 those ## elements that are different from k and stores those elements in the ## object train. ## For k=1, train consists of elements that are different from 1; that ## is 2, 3, …, n. m1=lm(GPM~.,data=FuelEff[train,]) pred=predict(m1,newdat=FuelEff[-train,]) #adding the new data, which is ignored earlier obs=FuelEff$GPM[-train] diff[k]=obs-pred percdiff[k]=abs(diff[k])/obs } me=mean(diff) rmse=sqrt(mean(diff**2)) mape=100*(mean(percdiff)) me # mean error rmse # root mean square error mape # mean absolute percent error ## cross-validation (leave one out) for the model on weight only n=length(FuelEff$GPM) diff=dim(n) percdiff=dim(n) for (k in 1:n) { train1=c(1:n) train=train1[train1!=k] m2=lm(GPM~WT,data=FuelEff[train,]) pred=predict(m2,newdat=FuelEff[-train,]) obs=FuelEff$GPM[-train] diff[k]=obs-pred percdiff[k]=abs(diff[k])/obs } me=mean(diff) rmse=sqrt(mean(diff**2)) mape=100*(mean(percdiff)) me # mean error rmse # root mean square error mape # mean absolute percent error  Graphs and Outcomes \n## regression on all data m1=lm(GPM~.,data=FuelEff) summary(m1) Call: lm(formula = GPM ~ ., data = FuelEff) Residuals: Min 1Q Median 3Q Max -0.4996 -0.2547 0.0402 0.1956 0.6455 Coefficients: Estimate Std. Error t value Pr(\u0026amp;gt;|t|) (Intercept) -2.599357 0.663403 -3.918 0.000458 *** WT 0.787768 0.451925 1.743 0.091222 . DIS -0.004890 0.002696 -1.814 0.079408 . NC 0.444157 0.122683 3.620 0.001036 ** HP 0.023599 0.006742 3.500 0.001431 ** ACC 0.068814 0.044213 1.556 0.129757 ET -0.959634 0.266785 -3.597 0.001104 ** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.313 on 31 degrees of freedom Multiple R-squared: 0.9386, Adjusted R-squared: 0.9267 F-statistic: 78.94 on 6 and 31 DF, p-value: \u0026lt; 2.2e-16 cor(FuelEff) GPM WT DIS NC HP ACC ET GPM 1.00000000 0.92626656 0.8229098 0.8411880 0.8876992 0.03307093 0.5206121 WT 0.92626656 1.00000000 0.9507647 0.9166777 0.9172204 -0.03357386 0.6673661 DIS 0.82290984 0.95076469 1.0000000 0.9402812 0.8717993 -0.14341745 0.7746636 NC 0.84118805 0.91667774 0.9402812 1.0000000 0.8638473 -0.12924363 0.8311721 HP 0.88769915 0.91722045 0.8717993 0.8638473 1.0000000 -0.25262113 0.7202350 ACC 0.03307093 -0.03357386 -0.1434174 -0.1292436 -0.2526211 1.00000000 -0.3102336 ET 0.52061208 0.66736606 0.7746636 0.8311721 0.7202350 -0.31023357 1.0000000 ## best subset regression in R library(leaps) X=FuelEff[,2:7] y=FuelEff[,1] out=summary(regsubsets(X,y,nbest=2,nvmax=ncol(X))) tab=cbind(out$which,out$rsq,out$adjr2,out$cp) tab (Intercept) WT DIS NC HP ACC ET 1 1 1 0 0 0 0 0 0.8579697 0.8540244 37.674750 1 1 0 0 0 1 0 0 0.7880098 0.7821212 72.979632 2 1 1 1 0 0 0 0 0.8926952 0.8865635 22.150747 2 1 1 0 0 0 0 1 0.8751262 0.8679906 31.016828 3 1 0 0 1 1 0 1 0.9145736 0.9070360 13.109930 3 1 1 1 1 0 0 0 0.9028083 0.8942326 19.047230 4 1 0 0 1 1 1 1 0.9313442 0.9230223 6.646728 4 1 1 0 1 1 0 1 0.9204005 0.9107520 12.169443 5 1 1 1 1 1 0 1 0.9337702 0.9234218 7.422476 5 1 0 1 1 1 1 1 0.9325494 0.9220103 8.038535 6 1 1 1 1 1 1 1 0.9385706 0.9266810 7.000000 m2=lm(GPM~WT,data=FuelEff) summary(m2) Call: lm(formula = GPM ~ WT, data = FuelEff) Residuals: Min 1Q Median 3Q Max -0.88072 -0.29041 0.00659 0.19021 1.13164 Coefficients: Estimate Std. Error t value Pr(\u0026amp;gt;|t|) (Intercept) -0.006101 0.302681 -0.02 0.984 WT 1.514798 0.102721 14.75 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.4417 on 36 degrees of freedom Multiple R-squared: 0.858, Adjusted R-squared: 0.854 F-statistic: 217.5 on 1 and 36 DF, p-value: \u0026lt; 2.2e-16  ","date":1570721445,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1570721445,"objectID":"54e428881124b61dde9eb6ea6f7421a7","permalink":"/vi/post/2019-10-10-fuel-efficiency/","publishdate":"2019-10-10T23:30:45+08:00","relpermalink":"/vi/post/2019-10-10-fuel-efficiency/","section":"post","summary":"This example shows a very simple regression analysis using fuel data of 38 cars. Code is copied from the book “Data Mining and Business Analytics with R” with minor modification on the graphs.\nThere are 2 learning points to be remembered with this example.\nLeaps package with regsubsets fuction. Regsubsets function allows to perform regression on subsets of data, particularly useful to compare across model’s predictors to understand the fit of the model.","tags":["R","Plot","Regression Analysis"],"title":"Fuel Efficiency - Regression Analysis","type":"post"},{"authors":null,"categories":null,"content":"In many cases, we need to remove space in the names of files saved inside a tree of folders/subfolders. We can use following CODE to do the job.\n:renameNoSpace [/R] @echo off setlocal disableDelayedExpansion if /i \u0026quot;%~1\u0026quot;==\u0026quot;/R\u0026quot; ( set \u0026quot;forOption=%~1 %2\u0026quot; set \u0026quot;inPath=\u0026quot; ) else ( set \u0026quot;forOption=\u0026quot; if \u0026quot;%~1\u0026quot; neq \u0026quot;\u0026quot; (set \u0026quot;inPath=%~1\\\u0026quot;) else set \u0026quot;inPath=\u0026quot; ) for %forOption% %%F in (\u0026quot;%inPath%* *\u0026quot;) do ( if /i \u0026quot;%~f0\u0026quot; neq \u0026quot;%%~fF\u0026quot; ( set \u0026quot;folder=%%~dpF\u0026quot; set \u0026quot;file=%%~nxF\u0026quot; setlocal enableDelayedExpansion echo ren \u0026quot;!folder!!file!\u0026quot; \u0026quot;!file: =!\u0026quot; ren \u0026quot;!folder!!file!\u0026quot; \u0026quot;!file: =!\u0026quot; endlocal ) )  Assume the script is called renameNoSpace.bat\nOptions to change are\nrenameNoSpace : (no arguments) Renames files in the current directory renameNoSpace /R : Renames files in the folder tree rooted at the current directory renameNoSpace myFolder : Renames files in the “myFolder” directory found in the current directory. renameNoSpace \u0026quot;c:\\my folder\\\u0026quot; : Renames files in the specified path. Quotes are used because path contains a space. renameNoSpace /R c:\\ : Renames all files on the C: drive.  We paste renameNoSpace.bat to targetted folder/subfolders and run the file in the console.\n","date":1570721445,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1570721445,"objectID":"dd2505d1189f072382cd00da3784c5bf","permalink":"/vi/post/2019-10-10-remove-space-in-bulk/","publishdate":"2019-10-10T23:30:45+08:00","relpermalink":"/vi/post/2019-10-10-remove-space-in-bulk/","section":"post","summary":"In many cases, we need to remove space in the names of files saved inside a tree of folders/subfolders. We can use following CODE to do the job.\n:renameNoSpace [/R] @echo off setlocal disableDelayedExpansion if /i \u0026quot;%~1\u0026quot;==\u0026quot;/R\u0026quot; ( set \u0026quot;forOption=%~1 %2\u0026quot; set \u0026quot;inPath=\u0026quot; ) else ( set \u0026quot;forOption=\u0026quot; if \u0026quot;%~1\u0026quot; neq \u0026quot;\u0026quot; (set \u0026quot;inPath=%~1\\\u0026quot;) else set \u0026quot;inPath=\u0026quot; ) for %forOption% %%F in (\u0026quot;%inPath%* *\u0026quot;) do ( if /i \u0026quot;%~f0\u0026quot; neq \u0026quot;%%~fF\u0026quot; ( set \u0026quot;folder=%%~dpF\u0026quot; set \u0026quot;file=%%~nxF\u0026quot; setlocal enableDelayedExpansion echo ren \u0026quot;!","tags":["Window"],"title":"Removing Space of files inside folders/subfolders in bulk","type":"post"},{"authors":null,"categories":null,"content":" This is an example presented in the book “Data Mining and Business Analytics with R”, with some useful basic graphs that can be reused for other sets of similar data. Code and Data are saved in Github link\nstore brand week logmove feat price AGE60 EDUC ETHNIC INCOME HHLARGE 1 2 tropicana 40 9.018695 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 2 2 tropicana 46 8.723231 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 3 2 tropicana 47 8.253228 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 4 2 tropicana 48 8.987197 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 5 2 tropicana 50 9.093357 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 6 2 tropicana 51 8.877382 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 7 2 tropicana 52 9.294682 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 8 2 tropicana 53 8.954674 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 9 2 tropicana 54 9.049232 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 10 2 tropicana 57 8.613230 0 3.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 11 2 tropicana 58 8.680672 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 12 2 tropicana 59 9.034080 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 13 2 tropicana 60 8.691483 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 14 2 tropicana 61 8.831712 0 3.56 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 15 2 tropicana 62 9.128696 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 16 2 tropicana 63 9.405907 0 2.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 17 2 tropicana 64 9.447150 0 2.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 18 2 tropicana 65 8.783856 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 19 2 tropicana 66 8.723231 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 20 2 tropicana 67 9.957976 0 2.39 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 21 2 tropicana 68 9.426741 0 2.39 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 22 2 tropicana 69 9.156095 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 23 2 tropicana 70 9.793673 0 2.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 24 2 tropicana 71 9.149316 0 2.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 25 2 tropicana 72 8.743851 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 26 2 tropicana 73 8.841014 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 27 2 tropicana 74 9.727228 0 2.49 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 28 2 tropicana 75 8.743851 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 29 2 tropicana 76 8.979165 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 30 2 tropicana 77 8.723231 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 31 2 tropicana 78 8.979165 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 32 2 tropicana 79 8.962904 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 33 2 tropicana 80 8.712760 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 34 2 tropicana 81 10.649607 1 1.69 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 35 2 tropicana 82 8.502689 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 36 2 tropicana 83 10.292281 1 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 37 2 tropicana 84 9.208739 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 38 2 tropicana 85 10.468801 1 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 39 2 tropicana 86 10.083139 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 40 2 tropicana 87 8.868413 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 41 2 tropicana 88 10.106918 1 2.29 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 42 2 tropicana 89 8.754003 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 43 2 tropicana 90 8.712760 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 44 2 tropicana 91 10.420375 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 45 2 tropicana 92 9.491602 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 46 2 tropicana 93 8.733594 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 47 2 tropicana 94 9.270871 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 48 2 tropicana 95 10.707102 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 49 2 tropicana 97 9.908276 0 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 50 2 tropicana 98 9.121728 1 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 51 2 tropicana 99 9.996614 0 2.19 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 52 2 tropicana 100 9.515469 0 2.19 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 53 2 tropicana 103 8.333270 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 54 2 tropicana 104 10.582130 1 1.99 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 55 2 tropicana 105 8.636220 0 3.59 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 56 2 tropicana 106 9.107643 1 2.68 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 57 2 tropicana 107 8.702178 0 3.44 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 58 2 tropicana 108 8.954674 0 3.14 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 WORKWOM HVAL150 SSTRDIST SSTRVOL CPDIST5 CPWVOL5 1 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 2 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 3 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 4 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 5 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 6 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 7 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 8 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 9 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 10 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 11 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 12 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 13 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 14 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 15 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 16 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 17 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 18 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 19 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 20 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 21 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 22 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 23 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 24 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 25 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 26 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 27 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 28 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 29 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 30 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 31 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 32 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 33 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 34 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 35 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 36 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 37 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 38 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 39 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 40 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 41 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 42 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 43 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 44 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 45 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 46 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 47 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 48 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 49 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 50 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 51 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 52 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 53 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 54 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 55 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 56 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 57 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 58 0.3035853 0.4638871 2.110122 1.142857 1.92728 0.3769266 [ reached 'max' / getOption(\u0026quot;max.print\u0026quot;) -- omitted 28889 rows ]  ## Install packages from CRAN; use any USA mirror library(lattice) #oj \u0026lt;- read.csv(\u0026quot;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/oj.csv\u0026quot;) oj \u0026lt;- read.csv(\u0026quot;oj.csv\u0026quot;) oj$store \u0026lt;- factor(oj$store) #change numberic value of store into categorical data oj[1:2,] t1=tapply(oj$logmove,oj$brand,FUN=mean,na.rm=TRUE) #calculate the mean of each brand using logmove value. t1 t2=tapply(oj$logmove,INDEX=list(oj$brand,oj$week),FUN=mean,na.rm=TRUE) #calculate the mean of logmove value based on index lists per week. t2 #plot each graph as time serieas data per week. plot.new() par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2),bg=\u0026quot;white\u0026quot;,cex = 1, cex.main = 0.6) plot(t2[1,],type= \u0026quot;l\u0026quot;,xlab=\u0026quot;week\u0026quot;,ylab=\u0026quot;dominicks\u0026quot;,ylim=c(7,12),cex.axis = 1,las = 1) plot(t2[2,],type= \u0026quot;l\u0026quot;,xlab=\u0026quot;week\u0026quot;,ylab=\u0026quot;minute.maid\u0026quot;,ylim=c(7,12),cex.axis = 1,las = 1) plot(t2[3,],type= \u0026quot;l\u0026quot;,xlab=\u0026quot;week\u0026quot;,ylab=\u0026quot;tropicana\u0026quot;,ylim=c(7,12),cex.axis = 1,las = 1) dev.copy(png,'oj_weekmean01.png',width = 1600, height = 600) dev.off() #------------------------------- #now we combine the three above graphs into one single graphs for ease of comparison logmove=c(t2[1,],t2[2,],t2[3,]) week1=c(40:160) week=c(week1,week1,week1) brand1=rep(1,121) brand2=rep(2,121) brand3=rep(3,121) brand=c(brand1,brand2,brand3) plot.new() xyplot(logmove~week|factor(brand),type= \u0026quot;l\u0026quot;,layout=c(1,3),col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_weekmean02.png',width = 1000, height = 600) dev.off() #----------------------------- plot.new() par(mfrow=c(1,1)) #par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2),bg=\u0026quot;white\u0026quot;,cex = 1, cex.main = 0.6) boxplot(logmove~brand,data=oj) # compare logmove of 3 branch using boxplot dev.copy(png,'oj_logmovebrandboxplot.png',width = 800, height = 600) dev.off() histogram(~logmove|brand,data=oj,layout=c(1,3)) # compare logmove of 3 branch using histogram dev.copy(png,'oj_logmovebrandhist.png',width = 1000, height = 600) dev.off() a1=densityplot(~logmove|brand,data=oj,layout=c(1,3),plot.points=FALSE) # compare logmove of 3 branch using density plot a2=densityplot(~logmove,groups=brand,data=oj,plot.points=FALSE) ## compare logmove of 3 branch using density plot in a one frame #using xyplot to see the spartial distribution of data weekly library(gridExtra) #this package allows to plot multiple graphs in the same plot despite the difference in plotting engines (e.g. ggplot or barchart) grid.arrange(a1, a2, ncol = 2) #display the two plot a and p dev.copy(png,'oj_logmovedensity.png',width = 1000, height = 500) dev.off() #------------------------------------------------- xyplot(logmove~week,data=oj,col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_logmoveweekspartial.png',width = 1000, height = 500) dev.off() #--------------------------------- xyplot(logmove~week|brand,data=oj,layout=c(1,3),col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_logmoveweekspartialbrand.png',width = 1000, height = 500) dev.off() #--------------------------------- xyplot(logmove~price,data=oj,col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_logmoveprice.png',width = 1000, height = 500) dev.off() #--------------------------------- xyplot(logmove~price|brand,data=oj,layout=c(1,3),col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_logmovepricebrand.png',width = 1000, height = 500) dev.off() #--------------------------------- smoothScatter(oj$price,oj$logmove) dev.copy(png,'oj_logmovepricesmooth.png',width = 1000, height = 500) dev.off() #--------------------------------- a1=densityplot(~logmove,groups=feat, data=oj, plot.points=FALSE) a2=xyplot(logmove~price,groups=feat, data=oj) grid.arrange(a1, a2, ncol = 2) #display the two plot a and p dev.copy(png,'oj_logmovepricegroupfeat.png',width = 1200, height = 500) dev.off() #------------------------------------------------ oj1=oj[oj$store == 5,] xyplot(logmove~week|brand,data=oj1,type=\u0026quot;l\u0026quot;,layout=c(1,3),col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_logmovebrand.png',width = 1200, height = 500) dev.off() xyplot(logmove~price,data=oj1,col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_logmovepricexyplot.png',width = 800, height = 500) dev.off() xyplot(logmove~price|brand,data=oj1,layout=c(1,3),col=\u0026quot;black\u0026quot;) dev.copy(png,'oj_logmovepricebrandxyplot.png',width = 1000, height = 500) dev.off() densityplot(~logmove|brand,groups=feat,data=oj1,plot.points=FALSE) dev.copy(png,'oj_logmovebranddenst.png',width = 1200, height = 500) dev.off() xyplot(logmove~price|brand,groups=feat,data=oj1) dev.copy(png,'oj_logmovepricebrandxyplot.png',width = 1200, height = 500) dev.off() #---------------------------- t21=tapply(oj$INCOME,oj$store,FUN=mean,na.rm=TRUE) t21 t21[t21==max(t21)] t21[t21==min(t21)] oj1=oj[oj$store == 62,] oj2=oj[oj$store == 75,] oj3=rbind(oj1,oj2) #---------------------------------------- a1=xyplot(logmove~price|store,data=oj3) a2=xyplot(logmove~price|store,groups=feat,data=oj3) grid.arrange(a1, a2, ncol = 1) #display the two plot a and p dev.copy(png,'oj_logmovexyplotprice.png',width = 500, height = 1000) dev.off() ## store in the wealthiest neighborhood plot.new() par(mar=c(4,4,1,1)+0.1,mfrow=c(1,2),bg=\u0026quot;white\u0026quot;,cex = 1, cex.main = 1) mhigh=lm(logmove~price,data=oj1) summary(mhigh) plot(logmove~price,data=oj1,xlim=c(0,4),ylim=c(0,13), main=\u0026quot;62 = wealthiest store\u0026quot;) abline(mhigh) ## store in the poorest neighborhood mlow=lm(logmove~price,data=oj2) summary(mlow) plot(logmove~price,data=oj2,xlim=c(0,4),ylim=c(0,13), main=\u0026quot;75 = poorest store\u0026quot;) abline(mlow) dev.copy(png,'oj_logmovepriceoj2.png',width = 1000, height = 300) dev.off()  Graphs Mean of logmove over 121 weeks\n\n\nboxplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe last two graphs also present the regression lines with summary of coefficients using lm function\nmhigh=lm(logmove~price,data=oj1) summary(mhigh) Call: lm(formula = logmove ~ price, data = oj1) Residuals: Min 1Q Median 3Q Max -4.9557 -0.4934 0.1815 0.6557 2.4454 Coefficients: Estimate Std. Error t value Pr(\u0026amp;gt;|t|) (Intercept) 9.15394 0.21112 43.359 \u0026lt;2e-16 *** price -0.01461 0.08381 -0.174 0.862 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1.142 on 349 degrees of freedom Multiple R-squared: 8.712e-05, Adjusted R-squared: -0.002778 F-statistic: 0.03041 on 1 and 349 DF, p-value: 0.8617 mlow=lm(logmove~price,data=oj2) summary(mlow) Call: lm(formula = logmove ~ price, data = oj2) Residuals: Min 1Q Median 3Q Max -3.5235 -0.5606 0.0392 0.5090 2.4523 Coefficients: Estimate Std. Error t value Pr(\u0026amp;gt;|t|) (Intercept) 10.87695 0.15184 71.63 \u0026lt;2e-16 *** price -0.67222 0.06071 -11.07 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.8383 on 352 degrees of freedom Multiple R-squared: 0.2584, Adjusted R-squared: 0.2563 F-statistic: 122.6 on 1 and 352 DF, p-value: \u0026lt; 2.2e-16  ","date":1570635045,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1570635045,"objectID":"d5e33e34da32df5a6cfbd62efe09c62a","permalink":"/vi/post/2019-10-09-orange-juice/","publishdate":"2019-10-09T23:30:45+08:00","relpermalink":"/vi/post/2019-10-09-orange-juice/","section":"post","summary":"This is an example presented in the book “Data Mining and Business Analytics with R”, with some useful basic graphs that can be reused for other sets of similar data. Code and Data are saved in Github link\nstore brand week logmove feat price AGE60 EDUC ETHNIC INCOME HHLARGE 1 2 tropicana 40 9.018695 0 3.87 0.2328647 0.2489349 0.1142799 10.55321 0.1039534 2 2 tropicana 46 8.723231 0 3.87 0.2328647 0.","tags":["R","Plot"],"title":"Orange Juice","type":"post"},{"authors":null,"categories":null,"content":"This is a summary of example 2 in Chapter 2 of the book “Data Mining and Business Analytics with R”. The post keeps the original code with some polishing syntax for better plotting, particularly using ggplot2 package and gridExtra.\nCode and Data are saved in Github link\nlibrary(lattice) library(ggplot2) #Using multiple plot function when using with ggplot #source(\u0026quot;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/genericfunctions/multiplot.R\u0026quot;) #----1. Data don \u0026lt;- read.csv(\u0026quot;contribution.csv\u0026quot;) #or read directly from the web #don \u0026lt;- read.csv(\u0026quot;https://www.biz.uiowa.edu/faculty/jledolter/DataMining/contribution.csv\u0026quot;) #or don[1:5,] #display the first 5 data rows table(don$Class.Year) #display total numbers of data points for each batch of year a=barchart(table(don$Class.Year),horizontal=FALSE,xlab=\u0026quot;Class Year\u0026quot;,col=\u0026quot;black\u0026quot;) p=ggplot(data.frame(table(don$Class.Year)), aes(x=Var1, y=Freq))+labs(y=\u0026quot;Freq\u0026quot;, x=\u0026quot;Class Year\u0026quot;) + geom_bar(stat=\u0026quot;identity\u0026quot;,width=0.8,color=\u0026quot;blue\u0026quot;,fill=\u0026quot;steelblue\u0026quot;)+geom_text(aes(label=Freq), vjust=-0.3, size=3.5) plot.new() par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(1,2),bg=\u0026quot;white\u0026quot;) library(gridExtra) #this package allows to plot multiple graphs in the same plot despite the difference in plotting engines (e.g. ggplot or barchart) grid.arrange(a, p, ncol = 2) #display the two plot a and p dev.copy(png,'alumni_classyear_bar.png',width = 1200, height = 500) dev.off()  \u0026gt; don[1:5,] Gender Class.Year Marital.Status Major Next.Degree FY04Giving FY03Giving FY02Giving 1 M 1957 M History LLB 2500 2500 1400 2 M 1957 M Physics MS 5000 5000 5000 3 F 1957 M Music NONE 5000 5000 5000 4 M 1957 M History NONE 0 5100 200 5 M 1957 M Biology MD 1000 1000 1000 FY01Giving FY00Giving AttendenceEvent 1 12060 12000 1 2 5000 10000 1 3 5000 10000 1 4 200 0 1 5 1005 1000 1 \u0026gt; table(don$Class.Year) 1957 1967 1977 1987 1997 127 222 243 277 361  Barchart from Lattice package\n\ndon$TGiving=don$FY00Giving+don$FY01Giving+don$FY02Giving+don$FY03Giving+don$FY04Giving mean(don$TGiving) sd(don$TGiving) quantile(don$TGiving,probs=seq(0,1,0.05)) quantile(don$TGiving,probs=seq(0.95,1,0.01))  mean(don$TGiving) [1] 980.0436 \u0026gt; sd(don$TGiving) [1] 6670.773 \u0026gt; quantile(don$TGiving,probs=seq(0,1,0.05)) 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 0.0 0.0 0.0 0.0 0.0 0.0 0.0 10.0 25.0 50.0 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 75.0 100.0 150.8 200.0 275.0 400.0 554.2 781.0 1050.0 2277.5 100% 171870.1 \u0026gt; quantile(don$TGiving,probs=seq(0.95,1,0.01)) 95% 96% 97% 98% 99% 100% 2277.50 3133.56 5000.00 7000.00 16442.14 171870.06  #--------------------- plot.new() par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2)) hist(don$TGiving,main=NULL,xlab=\u0026quot;Total Contribution\u0026quot;) #histograph with outliners hist(don$TGiving[don$TGiving!=0][don$TGiving[don$TGiving!=0]\u0026lt;=1000],main=NULL,xlab=\u0026quot;Total Contribution\u0026quot;) #histograph after delete outliners boxplot(don$TGiving,horizontal=TRUE,xlab=\u0026quot;Total Contribution\u0026quot;) #boxplot with outliners boxplot(don$TGiving,outline=FALSE,horizontal=TRUE,xlab=\u0026quot;Total Contribution\u0026quot;) #boxplot without outliners dev.copy(png,'alumni_contributionplot.png',width = 800, height = 500) dev.off()  \nddd=don[don$TGiving\u0026gt;=30000,] #seeing only total giving greater than 30K ddd ddd1=ddd[,c(1:5,12)] #display colum from 1 to 5 and column 12 ddd1 ddd1[order(ddd1$TGiving,decreasing=TRUE),] #display with decreasing #----------------- plot.new() par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2)) boxplot(TGiving~Class.Year,data=don,outline=FALSE, xlab=\u0026quot;year\u0026quot;) boxplot(TGiving~Gender,data=don,outline=FALSE, xlab=\u0026quot;sex\u0026quot;) boxplot(TGiving~Marital.Status,data=don,outline=FALSE,xlab=\u0026quot;Marital status\u0026quot;) boxplot(TGiving~AttendenceEvent,data=don,outline=FALSE,xlab=\u0026quot;Attend event or not\u0026quot;) dev.copy(png,'alumni_distribution_boxplot.png',width = 800, height = 500) dev.off()  \nplot.new() #----------------- t4=tapply(don$TGiving,don$Major,mean,na.rm=TRUE) t4 t5=table(don$Major) t5 t6=cbind(t4,t5) t7=t6[t6[,2]\u0026gt;10,] t7[order(t7[,1],decreasing=TRUE),] plot(barchart(t7[,1],col=\u0026quot;black\u0026quot;)) dev.copy(png,'alumni_major_barplot.png',width = 800, height = 500) dev.off()  \n#----------------- plot.new() t4=tapply(don$TGiving,don$Next.Degree,mean,na.rm=TRUE) t4 t5=table(don$Next.Degree) t5 t6=cbind(t4,t5) t7=t6[t6[,2]\u0026gt;10,] t7[order(t7[,1],decreasing=TRUE),] plot(barchart(t7[,1],col=\u0026quot;black\u0026quot;)) dev.copy(png,'alumni_degree_barplot.png',width = 800, height = 500) dev.off()  \n#----------------- plot.new() densityplot(~TGiving|factor(Class.Year),data=don[don$TGiving\u0026lt;=1000,][don[don$TGiving\u0026lt;=1000,]$TGiving\u0026amp;gt;0,],plot.points=FALSE,col=\u0026quot;black\u0026quot;) dev.copy(png,'alumni_year_densityplot.png',width = 800, height = 500) dev.off()  \nt11=tapply(don$TGiving,don$Class.Year,FUN=sum,na.rm=TRUE) t11 #----------------- plot.new() par(mfrow=c(1,1)) barplot(t11,ylab=\u0026quot;Average Donation\u0026quot;) dev.copy(png,'alumni_year_barplot.png',width = 800, height = 500) dev.off()  \n#----------------- plot.new() par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(2,2)) barchart(tapply(don$FY04Giving,don$Class.Year,FUN=sum, na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2004\u0026quot;) barchart(tapply(don$FY03Giving,don$Class.Year,FUN=sum, na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2003\u0026quot;) barchart(tapply(don$FY02Giving,don$Class.Year,FUN=sum, na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2002\u0026quot;) barchart(tapply(don$FY01Giving,don$Class.Year,FUN=sum, na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2001\u0026quot;) barchart(tapply(don$FY00Giving,don$Class.Year,FUN=sum, na.rm=TRUE),horizontal=FALSE,ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2000\u0026quot;) #same plot but with par #----------------- plot.new() par(mar=c(4.5,4.3,1,1)+0.1,mfrow=c(3,2),bg=\u0026quot;white\u0026quot;) barplot(tapply(don$FY04Giving,don$Class.Year,FUN=sum, na.rm=TRUE),ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2004\u0026quot;) barplot(tapply(don$FY03Giving,don$Class.Year,FUN=sum, na.rm=TRUE),ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2003\u0026quot;) barplot(tapply(don$FY02Giving,don$Class.Year,FUN=sum, na.rm=TRUE),ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2002\u0026quot;) barplot(tapply(don$FY01Giving,don$Class.Year,FUN=sum, na.rm=TRUE),ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2001\u0026quot;) barplot(tapply(don$FY00Giving,don$Class.Year,FUN=sum, na.rm=TRUE),ylim=c(0,225000),col=\u0026quot;black\u0026quot;, main=\u0026quot;2000\u0026quot;) dev.copy(png,'alumni_annual_barplot.png',width = 500, height = 800) dev.off()  \n#----------------- plot.new() par(mfrow=c(1,1)) don$TGivingIND=cut(don$TGiving,breaks=c(-1,0.5,10000000),labels=FALSE)-1 mean(don$TGivingIND) t5=table(don$TGivingIND,don$Class.Year) t5 barplot(t5,beside=TRUE) mosaicplot(factor(don$Class.Year)~factor(don$TGivingIND)) t50=tapply(don$TGivingIND,don$Class.Year,FUN=mean,na.rm=TRUE) t50 p3=barchart(t50,horizontal=FALSE,xlab=\u0026quot;Class Year\u0026quot;,col=\u0026quot;black\u0026quot;, main=\u0026quot;TGiving\u0026quot;) don$FY04GivingIND=cut(don$FY04Giving,c(-1,0.5,10000000),labels=FALSE)-1 t51=tapply(don$FY04GivingIND,don$Class.Year,FUN=mean,na.rm=TRUE) t51 p4=barchart(t51,horizontal=FALSE,xlab=\u0026quot;Class Year\u0026quot;,col=\u0026quot;black\u0026quot;, main=\u0026quot;FY04Giving\u0026quot;) grid.arrange(p3, p4, ncol = 2) dev.copy(png,'alumni_annual_barplotfreq.png',width = 800, height = 300) dev.off()  \n\n\n\n\n\n\n","date":1570548645,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1570548645,"objectID":"3c90810cb87907f9aaf6175cef70015e","permalink":"/vi/post/2019-10-08-alumni-contribution/","publishdate":"2019-10-08T23:30:45+08:00","relpermalink":"/vi/post/2019-10-08-alumni-contribution/","section":"post","summary":"This is a summary of example 2 in Chapter 2 of the book “Data Mining and Business Analytics with R”. The post keeps the original code with some polishing syntax for better plotting, particularly using ggplot2 package and gridExtra.\nCode and Data are saved in Github link\nlibrary(lattice) library(ggplot2) #Using multiple plot function when using with ggplot #source(\u0026quot;https://raw.githubusercontent.com/namkyodai/BusinessAnalytics/master/genericfunctions/multiplot.R\u0026quot;) #----1. Data don \u0026lt;- read.csv(\u0026quot;contribution.csv\u0026quot;) #or read directly from the web #don \u0026lt;- read.","tags":["R","Plot"],"title":"Alumni Contributions","type":"post"},{"authors":null,"categories":null,"content":" Earned Value Management (EVM) is a widely used method, if not to say, a imperative method in project management. Without the EVM, the Project Managers and stakeholders (PM) cannot track and monitor the Progress and Schedule, cannot understand the causes of problems and delays.\nIn a large scale engineering project, Contractors shall have an excellent and FULL TIME experienced Planners who can confidently perform EVM analysis and report to the PM and the project team weekly and monthly. The analysis report shall includes\n Updated Project Schedule with actual progress; Comparison between the Actual Progress vs Baselines; Critical Path Analysis (CPA) and identification of factors contributing to especially delays; The EVM.  \nIt seems that creating the graph like the one shown above is EASY. ^_^ Nope, it is not easy at all, but if it is not difficult if Planners understand the statistics and a smart way to work with compiling data.\nThis article presents 2 different ways to generate the EVM graphs.\nEVM for Consultancy Project using MS Project and R #Coded by Nam Le (namlt@prontonmail.ch) library(scales) library(reshape2) library(ggplot2) #----------DATA----------------- data \u0026lt;- read.csv(file=\u0026quot;Scurve.csv\u0026quot;, header=TRUE, sep=\u0026quot;,\u0026quot;) k=data.frame(data) # -----------------------------This needs to be modified whenever you plot plot.new() par(mar=c(4, 4, 4, 4), bg=\u0026quot;ivory\u0026quot;) week= length(data[!is.na(data[,3]),3]) #the lastest week that EV value is available. Or you can simply put the number of week in instead of using length(data[!is.na(data[,3]),3]). Here 3 is the column. scalefactor=100 trucy=100 ticktime=35 #Set the Project Week at which you want to limit the X-axis varrange=c(-30,30) #Posion of texbox (PV, EV, and VAR in the graph) htextbox=c(week-5,week) vtextbox=20 # -------------------------------- # ---------------------------- #Plot the baseline plot(data$BS01[1:ticktime],pch=4,axes=FALSE,ylim=c(0,trucy),ylab=\u0026quot;\u0026quot;, xlab=\u0026quot;\u0026quot;,col=\u0026quot;blue\u0026quot;,type=\u0026quot;o\u0026quot;,lwd=1,lty=1, main =paste(\u0026quot;S-curve of week\u0026quot;, week)) axis(2, ylim=c(0,trucy),col=\u0026quot;darkblue\u0026quot;,las=1) mtext(expression(paste(\u0026quot;Cummulative percentage (%)\u0026quot;)),side=2,line=2.2, adj = 0.5 ) axis(1,pretty(range(data$Week),30)) mtext(expression(paste(\u0026quot;Project Week\u0026quot;)),side=1,col=\u0026quot;black\u0026quot;,line=2.2) box() #plot the EV curve par(new=TRUE) plot(data$EV[1:ticktime],pch=18,axes=FALSE,ylim=c(0,trucy),ylab=\u0026quot;\u0026quot;, xlab=\u0026quot;\u0026quot;,col=\u0026quot;red\u0026quot;,type=\u0026quot;o\u0026quot;,lwd=1,lty=2) #adding stick at the actual week abline(v=week, col=\u0026quot;darkviolet\u0026quot;,lty=3) par(new=TRUE) #library(gplots) library(astro) #this package is for astronomy but it has some fantastic function for plotting. Here I use the textbox function textbox(htextbox, vtextbox, textlist=c(paste(\u0026quot;PV =\u0026quot;,format(round(data$BS01[week],2)),\u0026quot;EV =\u0026quot;,format(round(data$EV[week],2)),\u0026quot;VAR=\u0026quot;,format(round(data$EV[week]-data$BS01[week],2)))), justify='f', cex=0.7,col=\u0026quot;purple\u0026quot;, font=2, border=\u0026quot;green\u0026quot;, margin=-0.025,adj=0,box=1,fill=\u0026quot;aliceblue\u0026quot;) #alternatively, we can also use legend function for the same purpose, but I believe textbox function gives better look. #legend(30, 50, c(paste(\u0026quot;PV:\u0026quot;),format(round(data$BS01[week],2)),paste(\u0026quot;EV:\u0026quot;),format(round(data$EV[week],2)), paste(\u0026quot;VAR:\u0026quot;),format(round(data$BS01[week]-data$EV[week],2))), col=c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;),cex = 0.6) #plot the VAR curve using par(new=TRUE) plot((data$EV[1:ticktime]-data$BS01[1:ticktime]),pch=1,axes=FALSE,ylim=varrange,ylab=\u0026quot;\u0026quot;, xlab=\u0026quot;\u0026quot;,col=\u0026quot;cadetblue4\u0026quot;,type=\u0026quot;o\u0026quot;,lwd=1,lty=3) axis(4, ylim=varrange,col=\u0026quot;darkblue\u0026quot;,las=1) mtext(expression(paste(\u0026quot;Variance (%)\u0026quot;)),side=4,line=2.2, adj = 0.5 ) abline(h=0, col=\u0026quot;darkviolet\u0026quot;,lty=3) #plot the AC curve: This curve is added only for internal monitoring purpose. Shall not give this to the other stakeholders. par(new=TRUE) plot(data$AC[1:ticktime],pch=2,axes=FALSE,ylim=c(0,trucy),ylab=\u0026quot;\u0026quot;, xlab=\u0026quot;\u0026quot;,col=\u0026quot;violetred3\u0026quot;,type=\u0026quot;o\u0026quot;,lwd=1,lty=1) #axis(4, ylim=varrange,col=\u0026quot;darkblue\u0026quot;,las=1) #mtext(expression(paste(\u0026quot;Variance (%)\u0026quot;)),side=4,line=2.2, adj = 0.5 ) #abline(h=0, col=\u0026quot;darkviolet\u0026quot;,lty=3) #adding Planned Value, Earned Value, and Variance #adding legend legend(\u0026quot;topleft\u0026quot;, c(\u0026quot;PV\u0026quot;,\u0026quot;EV\u0026quot;, \u0026quot;VAR\u0026quot;, \u0026quot;AC\u0026quot;), text.col =\u0026quot;red\u0026quot;, border = \u0026quot;white\u0026quot;,box.lwd = 1,bg=\u0026quot;aliceblue\u0026quot;,lwd = 1,pch=c(4,18,1,2),lty =c(1,2,3,1), col=c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;cadetblue4\u0026quot;,\u0026quot;violetred3\u0026quot;),inset = .05,cex=0.8) #save png file dev.copy(png,'evm.png',width = 800, height = 500) dev.off() #-----THE END------------  \n","date":1570462245,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1570462245,"objectID":"a31032016f359a62c3a0f65bbbcd2902","permalink":"/vi/post/2019-10-07-evm/","publishdate":"2019-10-07T23:30:45+08:00","relpermalink":"/vi/post/2019-10-07-evm/","section":"post","summary":"Earned Value Management (EVM) is a widely used method, if not to say, a imperative method in project management. Without the EVM, the Project Managers and stakeholders (PM) cannot track and monitor the Progress and Schedule, cannot understand the causes of problems and delays.\nIn a large scale engineering project, Contractors shall have an excellent and FULL TIME experienced Planners who can confidently perform EVM analysis and report to the PM and the project team weekly and monthly.","tags":["EVM","Project Management","Large Scale Construction Project"],"title":"Earned Value Management","type":"post"},{"authors":null,"categories":null,"content":" This post describes a generic approach on the computation of Net Present Value (NPV) and Life Cycle Cost estimation (LCC) for Preventive Intervention (PI) on pumps (e.g. centrifugal pumps) when decision makers are uncertain on, or do not have a rich set of data on failure of pump components.\nIdeally, failure data on pump components should be recorded in time-series fashion that allows analyst to investigate on the frequency of failure and come up with a prediction. The prediction is powerful in view of generating budget for purchasing spare parts for not only one pump station but also for the entire network of pump stations. This is so-called Integrated Asset Management approach that should be at the center of an organization who has a long term view on how to sustain and prolong their assets.\nHowever, in many practical situations, failure data is limited or recorded inappropriately that does not become a useful set of information for analytic. In such a circumstance, decisions on whether to replace/rehabilitate pumps should be dependent on various uncertainty factors such as efficiency, assumption on average decaying rate of the assets, and most importantly the energy consumption vs the production produce.\nThis article presents a simple yet useful model for managers to make decisions on replacement of pumps if they are more or less aware of deficiency, low efficiency, and high ratio of energy and production over time.\nAside from the above, it is also worth to consider replacement of pumps when the two following aspects are existed.\n Asset obsolescence; Technological innovation.  There two aspects should be viewed in a combination. Asset obsolescence could be a result of technological innovation or changes in standards and requirements. Suffice it to state that pumps of a station were designed/installed more than a decade ago, nowadays, pumps are designed and manufactured to be more reliable and durable, whilst the capital cost is significant lower than that in the past.\nThe model Total Cost (TC) incurred in a period of T years is defined as the summation of Capital Cost \u0026copy; and Operation cost (O). It is note that operational cost includes routine maintenance cost.\n$$ TC(T) = \\sum{i=1}^{I} \\sum{t=1}^{T}\\frac{C{i,t}+O{i,t}\\times \\epsilon{i,t}^{new}\\times u{i,t}+(1-\\delta{i,t})\\times O{i,t}\\epsilon{i,t}^{old}\\times u{i,t}}{(1+\\rho)^t}$$\nIn the equation, $\\epsilon{i,t}$, $u{i,t}$, and $\\delta_{i,t}$ are the decreasing rate of efficiency, utilization, and decision variable of pump i in year t, respectively.\nExample A pump station has 6 booster pumps with the same configuration of 500 horsepower. The station has been in commission for more than 10 years and there has been a progressive degradation process, though it has not been captured appropriately. Sufficient failure data at component level is not available.\nTests have been conducted to measure the flow and the power rating that allows to come up efficiency of pump. However, due to non-optimal pipe configuration, the obtained efficiencies are also subjected to uncertainties.\nUnder such circumstance, decision makers have to make decisions under uncertainties.\n Discount factor is 8.5% annually; Capital cost for buying/installing a new pump of 500 horsepower would cost 5 millions Peso (~10,000 usd); Efficiency of existing pumps are about 5% less than that of the original design; Pumps are assumed to operate at 98% utilization level; Price of 1 KW is 6.5 Peso; There are 2 Intervention Strategies (IS) to be considerred. Herein refer to TC1 and TC2, respectively. TC1 is to replace existing pump with a new one in a step of 1 year. TC2 is Do Nothing, i.e., to keep the existing pumps as they are.  Source Code #this subroutine is used to estimate the cashflow of an investment for pump stations Npumps = 6 #total number of pumps TPeriod=10 #this infers 5 years TC1\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Option 1 TC2\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Option 2 CAPEX\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Captical investment cost OPEX\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #Operational Cost delta\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency epsilonold\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency epsilonnew\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #decrease in efficiency u\u0026lt;-matrix(nrow=TPeriod,ncol = Npumps) #ultilization rho=0.085 #discount factor pricee=6.5 hoursepower=500 utilization=0.98 Newpumpcost=5000000 #Option 1 - Stagging investment for pump for (t in 1: TPeriod){ if (t\u0026lt;2){ for (i in 1:Npumps){ epsilonold[t,i]\u0026lt;-1.05 } } else { for (i in 1:Npumps){ epsilonold[t,i]\u0026lt;-epsilonold[t-1,i]*1.05 } } } for (t in 1:TPeriod){ for (i in 1:Npumps){ delta[,]\u0026lt;-0 delta[1,1]\u0026lt;-1 epsilonnew[1,1]\u0026lt;-1 delta[2,2]\u0026lt;-1 epsilonnew[2,2]\u0026lt;-1 delta[3,3]\u0026lt;-1 epsilonnew[3,3]\u0026lt;-1 delta[4,4]\u0026lt;-1 epsilonnew[4,4]\u0026lt;-1 delta[5,5]\u0026lt;-1 epsilonnew[5,5]\u0026lt;-1 delta[6,6]\u0026lt;-1 epsilonnew[6,6]\u0026lt;-1 CAPEX[t,i]\u0026lt;-Newpumpcost #peso u[t,i]\u0026lt;-utilization OPEX[t,i]\u0026lt;- pricee*hoursepower*365*24*0.746*u[t,i] #peso } } for (t in 1: TPeriod){ for (i in 1:Npumps){ if (delta[t,i]==1){ epsilonnew[t,i]\u0026lt;-1 } else if (i==1 \u0026amp;amp; t==1){ epsilonnew[t,i]\u0026lt;-1 } else if (t\u0026amp;gt;1){ epsilonnew[t,i]\u0026lt;-epsilonnew[t-1,i]*1 } } } epsilonnew[is.na(epsilonnew)] \u0026lt;- 0 #redefine the value of epsilonold based on new value of epsilonnew for (t in 1: TPeriod){ for (i in 1:Npumps){ if (epsilonnew[t,i]\u0026lt;1){ epsilonold[t,i]\u0026lt;-epsilonold[t,i] } else if (epsilonnew[t,i]==1){ epsilonold[t,i]\u0026lt;-0 } } } for (i in 1:Npumps){ for (t in 1:TPeriod){ TC1[t,i]\u0026lt;-(((CAPEX[t,i]*delta[t,i]+OPEX[t,i]*epsilonnew[t,i]))+(1-delta[t,i])*OPEX[t,i]*epsilonold[t,i])/(1+rho)**t } } cat(\u0026quot;Replacement of pumps \\n\u0026quot;) print(TC1) #stop(\u0026quot;dd\u0026quot;) #Option 2 - Do Nothing for (t in 1:TPeriod){ for (i in 1:Npumps){ delta[,]\u0026lt;-0 CAPEX[t,i]\u0026lt;-0 #peso u[t,i]\u0026lt;-utilization epsilonnew[t,i]\u0026lt;-1.0 OPEX[t,i]\u0026lt;- pricee*hoursepower*365*24*0.746*u[t,i] #peso } } for (t in 1: TPeriod){ if (t\u0026lt;2){ for (i in 1:Npumps){ epsilonold[t,i]\u0026lt;-1.05 } } else { for (i in 1:Npumps){ epsilonold[t,i]\u0026lt;-epsilonold[t-1,i]*1.05 } } } for (i in 1:Npumps){ for (t in 1:TPeriod){ TC2[t,i]\u0026lt;-(((CAPEX[t,i]+OPEX[t,i]*epsilonnew[t,i])*delta[t,i])+(1-delta[t,i])*OPEX[t,i]*epsilonold[t,i])/(1+rho)**t } } cat(\u0026quot;Do Nothing \\n\u0026quot;) print(TC2) cat(\u0026quot;The difference of investment \\n\u0026quot;) print(TC1-TC2) #plot the graph for comparison library(ggplot2) time=c(1:TPeriod) x\u0026lt;-data.frame(dat=TC1[,1],IS=rep(\u0026quot;TC1\u0026quot;)) y\u0026lt;-data.frame(dat=TC2[,1],IS=rep(\u0026quot;TC2\u0026quot;)) x\u0026lt;-cbind(time,x) y\u0026lt;-cbind(time,y) xy\u0026lt;- rbind(x, y) ggplot(xy, aes(fill=IS, y=dat, x=factor(time))) + geom_bar(position=\u0026quot;dodge\u0026quot;, stat=\u0026quot;identity\u0026quot;) stop(\u0026quot;\u0026quot;) TC1\u0026lt;-data.frame(TC1,IS=rep(\u0026quot;TC1\u0026quot;)) TC2\u0026lt;-data.frame(TC2,IS=rep(\u0026quot;TC2\u0026quot;)) #TC2\u0026lt;-data.frame(TC2=TC2[,1]) #TC1\u0026lt;-data.frame(TC1) #TC2\u0026lt;-data.frame(TC2) TC1\u0026lt;-cbind(time,TC1) TC2\u0026lt;-cbind(time,TC2) library(reshape) mdataTC1 \u0026lt;- melt(TC1,id=c(\u0026quot;time\u0026quot;,\u0026quot;IS\u0026quot;)) mdataTC2 \u0026lt;- melt(TC2,id=c(\u0026quot;time\u0026quot;,\u0026quot;IS\u0026quot;)) #joint the two data total \u0026lt;- rbind(mdataTC1, mdataTC2) ggplot(total, aes(fill=IS, y=value, x=factor(time)),variable=X1) + geom_bar(position=\u0026quot;dodge\u0026quot;, stat=\u0026quot;identity\u0026quot;) #ggplot(total, aes(fill=variable, y=value, x=factor(time)),variable=X1) + geom_bar(position=\u0026quot;fill\u0026quot;, stat=\u0026quot;identity\u0026quot;) #ggplot(total, aes(y=value, x=variable, color=IS, fill=IS)) + geom_bar( stat=\u0026quot;identity\u0026quot;) + facet_wrap(~factor(time))  Results Estimation results are shown in following tables\n source(\u0026lsquo;C:/Dropbox/workspace/RProjects/PlantAudit/PAG/tc.R\u0026rsquo;) Replacement of pumps\n[,1] [,2] [,3] [,4] [,5] [,6] [1,] 23791565 20142433 20142433 20142433 20142433 20142433 [2,] 17680433 21927709 19492677 19492677 19492677 19492677 [3,] 16295330 16295330 20209870 18863881 18863881 18863881 [4,] 15018737 15018737 15018737 18626609 18255369 18255369 [5,] 13842154 13842154 13842154 13842154 17167381 17666486 [6,] 12757746 12757746 12757746 12757746 12757746 15822471 [7,] 11758291 11758291 11758291 11758291 11758291 11758291 [8,] 10837135 10837135 10837135 10837135 10837135 10837135 [9,] 9988142 9988142 9988142 9988142 9988142 9988142 [10,] 9205661 9205661 9205661 9205661 9205661 9205661 Do Nothing [,1] [,2] [,3] [,4] [,5] [,6] [1,] 20142433 20142433 20142433 20142433 20142433 20142433 [2,] 19492677 19492677 19492677 19492677 19492677 19492677 [3,] 18863881 18863881 18863881 18863881 18863881 18863881 [4,] 18255369 18255369 18255369 18255369 18255369 18255369 [5,] 17666486 17666486 17666486 17666486 17666486 17666486 [6,] 17096599 17096599 17096599 17096599 17096599 17096599 [7,] 16545096 16545096 16545096 16545096 16545096 16545096 [8,] 16011383 16011383 16011383 16011383 16011383 16011383 [9,] 15494887 15494887 15494887 15494887 15494887 15494887 [10,] 14995052 14995052 14995052 14995052 14995052 14995052 The difference of investment [,1] [,2] [,3] [,4] [,5] [,6] [1,] 3649131 0 0 0.0 0.0 0 [2,] -1812244 2435032 0 0.0 0.0 0 [3,] -2568551 -2568551 1345989 0.0 0.0 0 [4,] -3236632 -3236632 -3236632 371239.7 0.0 0 [5,] -3824332 -3824332 -3824332 -3824332.0 -499104.8 0 [6,] -4338854 -4338854 -4338854 -4338853.7 -4338853.7 -1274128 [7,] -4786805 -4786805 -4786805 -4786805.2 -4786805.2 -4786805 [8,] -5174249 -5174249 -5174249 -5174248.9 -5174248.9 -5174249 [9,] -5506745 -5506745 -5506745 -5506744.7 -5506744.7 -5506745 [10,] -5789391 -5789391 -5789391 -5789390.9 -5789390.9 -5789391  As can be seen from the table of difference, if pump 1 is to be replaced (TC1) now (year 1), the Owner needs to spend 5 millions Peso for the pump, plus the energy cost for that year, the total cost in year 1 would be 23,791,565 Peso. Meanwhile, if pump 1 is to follow TC2 (Do Nothing), there is a cost of 20,142,433 Peso, which is basically the energy cost. The difference between TC1 and TC2 in year 1 is 3,649,131 Peso, meaning the Owner will spend more money in year 1. In other words, there is a negative cash flow in the first year.\n However, from 2nd year onward, there is a decreasing in energy consumption, this is thanks to the higher efficiency pump that consume less energy. As a result, there is a saving of -1,812,244 Peso. This value has been discounted and it is the Net Present Value of the OPEX incurred in year two. This difference can be notably seen in the following graph. This graph clearly shows that more energy in following years will be saved if the pump 1 is to be replaced now. It can also be dictated that the amount of cost associated with the saving in energy will compensate the CAPEX within 2 years of investment.\n\n","date":1569771045,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1569771045,"objectID":"2e84fd1494e8b81846aaea0cc1243cb5","permalink":"/vi/post/2019-09-29-pump-efficiency/","publishdate":"2019-09-29T23:30:45+08:00","relpermalink":"/vi/post/2019-09-29-pump-efficiency/","section":"post","summary":"This post describes a generic approach on the computation of Net Present Value (NPV) and Life Cycle Cost estimation (LCC) for Preventive Intervention (PI) on pumps (e.g. centrifugal pumps) when decision makers are uncertain on, or do not have a rich set of data on failure of pump components.\nIdeally, failure data on pump components should be recorded in time-series fashion that allows analyst to investigate on the frequency of failure and come up with a prediction.","tags":["Pump efficiency","Pump Replacement","Asset Management"],"title":"Return on Investment (ROI) for Intervention of Pumps under uncertainties","type":"post"},{"authors":null,"categories":null,"content":" What if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F.. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.\nWhat if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F\u0026hellip;. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.\nLet start with time series data that is saved in hundreds of excel worksheets, examples are\n Production and consumption data of a single day. Within a day, volumes of production and consumption (e.g. water and energy of a water treatment plant or pump station, number of X and Y produced within a single hours); Hydrology data such as rainfall and runoff and other associated parameters etc  Ideally, those data should be recorded using relational database structured system such as MySQL or PostgreSQL. However, by default in many organizations, data is recorded in excel file. Everyday, owner of the file just multiply/copy the same worksheet of previous day and repeat the same work. This is OK for him/her but definitely not OK for us, the analyst :).\nThis post describes a step by step instruction on how to deal with this issue.\nExamples Data Example data is with two excel files name data1 and data2. These two files have their structure identical as shown in the followings\ndata1 worksheet1 date from to total_pro_hour total_power_hour ratio 01-Jan-17 12:00 AM1:00 AM 0.7700000 95 123.38 01-Jan-17 1:00 AM 2:00 AM 0.7400000 86 116.22 01-Jan-17 2:00 AM 3:00 AM 0.6200000 73 117.74 01-Jan-17 3:00 AM 4:00 AM 0.6100000 70 114.75 01-Jan-17 4:00 AM 5:00 AM 0.5700000 62 108.77 01-Jan-17 5:00 AM 6:00 AM 0.7300000 86 117.81 01-Jan-17 6:00 AM 7:00 AM 0.6900000 70 101.45 01-Jan-17 7:00 AM 8:00 AM 1.0100000 93 92.08 01-Jan-17 8:00 AM 9:00 AM 0.9100000 116 127.47 01-Jan-17 9:00 AM 10:00AM 1.0800000 120 111.11 01-Jan-17 10:00 AM 11:00AM 1.0900000 122 111.93 01-Jan-17 11:00 AM 12:00PM 1.0800000 119 110.19 01-Jan-17 12:00 PM 1:00PM 1.1000000 117 106.36 01-Jan-17 1:00 PM 2:00 PM 1.5420000 145 94.03 01-Jan-17 2:00 PM 3:00 PM 0.9330000 136 145.77 01-Jan-17 3:00 PM 4:00 PM 1.0520000 137 130.23 01-Jan-17 4:00 PM 5:00 PM 0.9600000 153 159.38 01-Jan-17 5:00 PM 6:00 PM 0.9910000 135 136.23 01-Jan-17 6:00 PM 7:00 PM 1.0110000 146 144.41 01-Jan-17 7:00 PM 8:00 PM 0.9320000 134 143.78 01-Jan-17 8:00 PM 9:00 PM 0.9680000 133 137.40 01-Jan-17 9:00 PM 10:00 PM 0.7110000 124 174.40 01-Jan-17 10:00 PM 11:00 PM 0.7800000 72 92.31 01-Jan-17 11:00 PM 12:00 AM 0.6100000 75 122.95 worksheet 2 02-Jan-17 12:00 AM 1:00 AM 0.5600000 48 85.71 02-Jan-17 1:00 AM 2:00 AM 0.4420000 42 95.02 02-Jan-17 2:00 AM 3:00 AM 0.4700000 40 85.11 02-Jan-17 3:00 AM 4:00 AM 0.6980000 59 84.53 02-Jan-17 4:00 AM 5:00 AM 0.8200000 86 104.88 02-Jan-17 5:00 AM 6:00 AM 0.4700000 48 102.13 02-Jan-17 6:00 AM 7:00 AM 1.0400000 121 116.35 02-Jan-17 7:00 AM 8:00 AM 1.0800000 146 135.19 02-Jan-17 8:00 AM 9:00 AM 1.0800000 122 112.96 02-Jan-17 9:00 AM 10:00 AM 0.9600000 82 85.42 02-Jan-17 10:00 AM 11:00 AM 0.9100000 73 80.22 02-Jan-17 11:00 AM 12:00 PM 0.8500000 65 76.47 02-Jan-17 12:00 PM 1:00 PM 0.7100000 57 80.28 02-Jan-17 1:00 PM 2:00 PM 0.9690000 48 49.54 02-Jan-17 2:00 PM 3:00 PM 0.8310000 65 78.22 02-Jan-17 3:00 PM 4:00 PM 1.1290000 96 85.03 02-Jan-17 4:00 PM 5:00 PM 1.2300000 109 88.62 02-Jan-17 5:00 PM 6:00 PM 1.1210000 114 101.69 02-Jan-17 6:00 PM 7:00 PM 0.9440000 110 116.53 02-Jan-17 7:00 PM 8:00 PM 0.9790000 112 114.40 02-Jan-17 8:00 PM 9:00 PM 1.0260000 108 105.26 02-Jan-17 9:00 PM 10:00 PM 0.8710000 118 135.48 02-Jan-17 10:00 PM 11:00 PM 0.8100000 99 122.22 02-Jan-17 11:00 PM 12:00 AM 0.6800000 79 116.18  This excel file contains 2 worksheet “1” and “2” that record hourly production (ML) and energy consumption (KW) of a water pump station.\n column is the date; column 2 and 3 are hours; column 4 is production data in million liter (ML); column 5 is energy consumption data in KW column 6 is ratio between energy consumption and production, basically it is the division of column 5 and 4.  This kind of data is recorded hourly and there will be about 30 worksheets for one month. Let say you have 5 or 10 years production data saving in excel files like this and you need to combine all of them into a single frame for Business Analysis purpose. It will be a nightmare if you just copy and paste 🙂 terrible excel.\nAssumptions It is assumed that all worksheets in all excel files are identical in their structure. This is not a realistic assumption as excel users cannot be consistent with their data. Data in their excel files are\nMixed up with numeric and text even for the same attributes; Merging cells, adding new rows and columns that make them homogeneous. Solving such problem is not the objective of this post, but it is worth to mention that before we get a good set of data, we probably need to do some Coding in Visual Basic to standardize the excel worksheets, or we need to do a certain level of manual data compiling before we can run the code in R.\nWill cover how to standardize using the same sets of data in other post.\nCombining worksheets using Navicat and MySQL Why NAVICAT?\n–\u0026gt; Navicat offers a handy way to import data from excel files. It can import multiple worksheets in one single click into respective tables of MySQL. I find this feature superior than other open source SQL Client such PhPmyAdmin, Dbeaver, etc.\nHerein, I demonstrate some steps to import the example data files.\na. Create table\ncreate table `1` ( date varchar(255), `from` varchar(255), `to` varchar(255), total_pro_hour varchar(255), total_power_hour varchar(255), ratio varchar(255) ); create table `2` as select * from `1`; create table `3` as select * from `1`; create table `4` as select * from `1`; create table `5` as select * from `1`; create table `6` as select * from `1`; create table `7` as select * from `1`; create table `8` as select * from `1`; create table `9` as select * from `1`; create table `10` as select * from `1`; create table `11` as select * from `1`; create table `12` as select * from `1`; create table `13` as select * from `1`; create table `14` as select * from `1`; create table `15` as select * from `1`; create table `16` as select * from `1`; create table `17` as select * from `1`; create table `18` as select * from `1`; create table `19` as select * from `1`; create table `20` as select * from `1`; create table `21` as select * from `1`; create table `22` as select * from `1`; create table `23` as select * from `1`; create table `24` as select * from `1`; create table `25` as select * from `1`; create table `26` as select * from `1`; create table `27` as select * from `1`; create table `28` as select * from `1`; create table `29` as select * from `1`; create table `30` as select * from `1`;  This SQL creates 30 tables name from 1 to 30 that correspond to each day in a month. Note that for February, there are 28 days but dont worry, we still use 30 or 31 worksheets as additional worksheets will be blank anyway and make no harm to the operation.\nb. Manual importing worksheets from the excel file to MySQL table using NaviCAT\n Select table 1 in MySQL database Right Click –\u0026gt; Import Wizard Select Excel file and Click Next Import the excel file  \n\n\n\n\n\nNow you have all tables you need in MySQL. However, Values of time shown in FROM and TO column have change from AM, PM to something else. For example 1900-01-02 should be 24. To solve this issue, we will use the following SQL syntax\nc. Rename tables\nRENAME TABLE `1` TO pat_2018_10_1, `2` TO pat_2018_10_2, `3` TO pat_2018_10_3, `4` TO pat_2018_10_4, `5` TO pat_2018_10_5, `6` TO pat_2018_10_6, `7` TO pat_2018_10_7, `8` TO pat_2018_10_8, `9` TO pat_2018_10_9, `10` TO pat_2018_10_10, `11` TO pat_2018_10_11, `12` TO pat_2018_10_12, `13` TO pat_2018_10_13, `14` TO pat_2018_10_14, `15` TO pat_2018_10_15, `16` TO pat_2018_10_16, `17` TO pat_2018_10_17, `18` TO pat_2018_10_18, `19` TO pat_2018_10_19, `20` TO pat_2018_10_20, `21` TO pat_2018_10_21, `22` TO pat_2018_10_22, `23` TO pat_2018_10_23, `24` TO pat_2018_10_24, `25` TO pat_2018_10_25, `26` TO pat_2018_10_26, `27` TO pat_2018_10_27, `28` TO pat_2018_10_28, `29` TO pat_2018_10_29, `30` TO pat_2018_10_30 ;  Here we rename the table to whatever we need. By doing so, we can resue the Create Query to perform the same procedure for importing new table.\nd. Create Production table\ncreate table production ( date varchar(255), `from` varchar(255), `to` varchar(255), total_pro_hour varchar(255), total_power_hour varchar(255), ratio varchar(255) );  This code creates a production table.\ne. Import/Append all raw tables into one table – Production\nINSERT INTO production SELECT * FROM pat_2017_1_1; INSERT INTO production SELECT * FROM pat_2017_1_2; INSERT INTO production SELECT * FROM pat_2017_1_3; INSERT INTO production SELECT * FROM pat_2017_1_4; INSERT INTO production SELECT * FROM pat_2017_1_5; INSERT INTO production SELECT * FROM pat_2017_1_6; INSERT INTO production SELECT * FROM pat_2017_1_7; INSERT INTO production SELECT * FROM pat_2017_1_8; INSERT INTO production SELECT * FROM pat_2017_1_9; INSERT INTO production SELECT * FROM pat_2017_1_10; INSERT INTO production SELECT * FROM pat_2017_1_11; INSERT INTO production SELECT * FROM pat_2017_1_12; INSERT INTO production SELECT * FROM pat_2017_1_13; INSERT INTO production SELECT * FROM pat_2017_1_14; INSERT INTO production SELECT * FROM pat_2017_1_15; INSERT INTO production SELECT * FROM pat_2017_1_16; INSERT INTO production SELECT * FROM pat_2017_1_17; INSERT INTO production SELECT * FROM pat_2017_1_18; INSERT INTO production SELECT * FROM pat_2017_1_19; INSERT INTO production SELECT * FROM pat_2017_1_20; INSERT INTO production SELECT * FROM pat_2017_1_21; INSERT INTO production SELECT * FROM pat_2017_1_22; INSERT INTO production SELECT * FROM pat_2017_1_23; INSERT INTO production SELECT * FROM pat_2017_1_24; INSERT INTO production SELECT * FROM pat_2017_1_25; INSERT INTO production SELECT * FROM pat_2017_1_26; INSERT INTO production SELECT * FROM pat_2017_1_27; INSERT INTO production SELECT * FROM pat_2017_1_28; INSERT INTO production SELECT * FROM pat_2017_1_29; INSERT INTO production SELECT * FROM pat_2017_1_30;  This code imports all data from raw table into production table.\nf. Correct data, particularly with date and time\nDROP TABLE IF EXISTS productioncorrected; create table productioncorrected select * from production; UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'1900-01-01','24:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'1900-01-02 ',''); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'1900-01-02','00:00:00.000'); ##### UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'1:00AM','01:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'2:00AM','02:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'3:00AM','03:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'4:00AM','04:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'5:00AM','05:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'6:00AM','06:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'7:00AM','07:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'8:00AM','08:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'9:00AM','09:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'10:00AM','10:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'11:00AM','11:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'12:00PM','12:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'01:00PM','13:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'02:00PM','14:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'03:00PM','15:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'04:00PM','16:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'05:00PM','17:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'06:00PM','18:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'07:00PM','19:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'08:00PM','20:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'09:00PM','21:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'10:00PM','22:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'11:00PM','23:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,'12:00AM','00:00:00.000'); ##### UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'1:00AM','01:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'2:00AM','02:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'3:00AM','03:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'4:00AM','04:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'5:00AM','05:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'6:00AM','06:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'7:00AM','07:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'8:00AM','08:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'9:00AM','09:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'10:00AM','10:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'11:00AM','11:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'12:00PM','12:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'01:00PM','13:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'02:00PM','14:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'03:00PM','15:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'04:00PM','16:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'05:00PM','17:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'06:00PM','18:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'07:00PM','19:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'08:00PM','20:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'09:00PM','21:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'10:00PM','22:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'11:00PM','23:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,'12:00AM','24:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,' 101:00:00.000','11:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = replace(productioncorrected.`to`,' 102:00:00.000','24:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,' 101:00:00.000','11:00:00.000'); UPDATE productioncorrected SET productioncorrected.`from` = replace(productioncorrected.`from`,' 102:00:00.000','24:00:00.000'); UPDATE productioncorrected SET productioncorrected.`to` = \u0026quot;24:00:00.000\u0026quot; WHERE productioncorrected.`from`= \u0026quot;23:00:00.000\u0026quot;; UPDATE productioncorrected SET productioncorrected.`to` = \u0026quot;11:00:00.000\u0026quot; WHERE productioncorrected.`from`= \u0026quot;10:00:00.000\u0026quot;; UPDATE productioncorrected SET productioncorrected.`from` = \u0026quot;11:00:00.000\u0026quot; WHERE productioncorrected.`to`= \u0026quot;12:00:00.000\u0026quot;; UPDATE productioncorrected SET productioncorrected.`from` = \u0026quot;24:00:00.000\u0026quot; WHERE productioncorrected.`to`= \u0026quot;01:00:00.000\u0026quot;;  This code is used to correct\n time data missing data outliner  g. Create analysis table\nDROP TABLE IF EXISTS analysis; CREATE TABLE analysis Select * from productioncorrected where productioncorrected.total_pro_hour \u0026gt;0 and productioncorrected.total_power_hour\u0026gt;0 and productioncorrected.total_power_hour NOT LIKE \u0026quot;-\u0026quot; ; ALTER TABLE analysis MODIFY COLUMN total_power_hour DOUBLE; ALTER TABLE analysis MODIFY COLUMN total_pro_hour DOUBLE; ALTER TABLE analysis MODIFY COLUMN ratio DOUBLE;  Analysis table is basically the same with production but with some correction\nh. Create Analysis Date table\nDROP TABLE IF EXISTS analysisdate; CREATE TABLE analysisdate Select * from analysis GROUP BY analysis.date; ALTER TABLE analysisdate ADD id int NOT NULL AUTO_INCREMENT PRIMARY KEY;  This SQL code will create Analysis Date table that basically summary hourly data into daily data.\n WHAO, after going through each of the above Query, my figures are tired already. You can further enhance SQL code to make the entire process faster, but still it is much slower than using below R code.\n Combining worksheets using R Below R codes are extracted from Github source\nThis code is used to combine multiple excel worksheets into one dataframe. Particularly useful when combining multiple worksheets of production data with each worksheet is a date of a month, and in each worksheet, data is saved in hourly basis.\nthe first method, using XlConnect.\n# this method has a limitation that XlConnect doesnt work well with excel files with dynamic links. When importing into R, it gives NA values. # to make sure that importing is perfect. It is advisable to disable all dynamic links in excel file by going to Data library(XLConnect) # load data file (excel files ended with cls, xlsc, or xlsm) datafile \u0026lt;- loadWorkbook(\u0026quot;data1.xlsx\u0026quot;) # This is a static worksheet, without any dynamic links # obtain sheet names worksheets \u0026lt;- getSheets(datafile) names(worksheets) \u0026lt;- worksheets # dataframe worksheets_list \u0026lt;- lapply(worksheets, function(.sheet){readWorksheet(object=datafile, .sheet)}) # limit worksheet_list to sheets with at least 1 dimension worksheets_list2 \u0026lt;- worksheets_list[sapply(worksheets_list, function(x) dim(x)[1]) \u0026amp;gt; 0] # code to read in each excel worksheet as individual dataframes for (i in 2:length(worksheets_list2)){assign(paste0(\u0026quot;df\u0026quot;, i), as.data.frame(worksheets_list2[i]))} # define function to clean data in each data frame (updated based on your data). You must define here carefully otherwise it will not work well with some certain type of data. The fastest way is only drop out missing values. Other value can be dealed with using query in MySQL cleaner \u0026lt;- function(df){ # drop rows with missing values df \u0026lt;- df[rowSums(is.na(df)) == 0,] # remove serial comma from all variables # df[,-1] \u0026lt;- as.numeric(gsub(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;, as.matrix(df[,-1]))) # create numeric version of year variable for graphing # df$Year \u0026lt;- as.numeric(substr(df$year, 1, 4)) # return cleaned df return(df) } # clean sheets and create one data frame data1 \u0026lt;- do.call(rbind,lapply(names(worksheets_list2), function(x) cleaner(worksheets_list2[[x]]))) cat(\u0026quot;Print out the data 1 frame \\n\u0026quot;) print(data1)  # Method is with readxl package. This is superior than the former one as readxl can handle excel files with dynamic links. This means it will retain values and ignore the links. # ---------------- library(readxl) read_excel_allsheets \u0026lt;- function(filename, tibble = FALSE) { # I prefer straight data.frames # but if you like tidyverse tibbles (the default with read_excel) # then just pass tibble = TRUE sheets \u0026lt;- readxl::excel_sheets(filename) x \u0026lt;- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X)) if(!tibble) x \u0026lt;- lapply(x, as.data.frame) names(x) \u0026lt;- sheets x } #start to read and write data into csv file worksheets \u0026lt;- read_excel_allsheets(\u0026quot;data1.xlsx\u0026quot;) source(\u0026quot;cleaning.R\u0026quot;) filedata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]]))) write.table(filedata, \u0026quot;myDF.csv\u0026quot;, sep = \u0026quot;,\u0026quot;, col.names = !file.exists(\u0026quot;myDF.csv\u0026quot;), row.names=FALSE, append = T) worksheets \u0026lt;- read_excel_allsheets(\u0026quot;data2.xlsm\u0026quot;) source(\u0026quot;cleaning.R\u0026quot;) filedata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]]))) write.table(filedata, \u0026quot;myDF.csv\u0026quot;, sep = \u0026quot;,\u0026quot;, col.names = !file.exists(\u0026quot;myDF.csv\u0026quot;), row.names=FALSE, append = T) #end #https://medium.com/@niharika.goel/merge-multiple-csv-excel-files-in-a-folder-using-r-e385d962a90a  Combining multiple excel files using R I found the code from Niharika suits the purpose of this exercise. Kindly refer to her github site for original code.\nhttps://github.com/NiharikaGoel12/R-Playground\n Her readme file states This repository contains basic codes for R, which might be useful in day to day work, especially doing data analysis on large datasets in Excel or CSV.\n#Merge multiple Excel/CSV files in a folder\nConsider a case when you have multiple xlsx/csv files in a folder \u0026amp; you to merge them into one single file. Here, I have used lapply() which returns a list of the same length as i. And grepl() will check exact match between merge_file_name \u0026amp; existing file ‘i’. In this case, if the two files are same, we will ignore already created “merge file”.\nrbind() will combine data frame by rows and merge all the files.\n a. Combining CSV files\npath \u0026lt;- \u0026quot;sample-data/merge-files/csv\u0026quot; merge_file_name \u0026lt;- \u0026quot;sample-data/merge-files/merged_file.csv\u0026quot; filenames \u0026lt;- list.files(path= path, full.names=TRUE) All \u0026lt;- lapply(filenames,function(filename){ print(paste(\u0026quot;Merging\u0026quot;,filename,sep = \u0026quot; \u0026quot;)) read.csv(filename) }) df \u0026lt;- do.call(rbind.data.frame, All) write.csv(df,merge_file_name)  b. Combining excel files\nlibrary(openxlsx) path \u0026lt;- \u0026quot;sample-data/merge-files/xlsx\u0026quot; merge_file_name \u0026lt;- \u0026quot;sample-data/merge-files/merged_file.xlsx\u0026quot; filenames_list \u0026lt;- list.files(path= path, full.names=TRUE) All \u0026lt;- lapply(filenames_list,function(filename){ print(paste(\u0026quot;Merging\u0026quot;,filename,sep = \u0026quot; \u0026quot;)) read.xlsx(filename) }) df \u0026lt;- do.call(rbind.data.frame, All) write.xlsx(df,merge_file_name)  c. Combining the example data\nWe can use the code in step a and b of this section. However, there is one draw back that we need to save our data first into csv or excel file. This is also a bit of time consuming. To avoid this, we can just simply write directly data into csv file as presented in the last part of section 4.\n# ---------------- library(readxl) read_excel_allsheets \u0026lt;- function(filename, tibble = FALSE) { # I prefer straight data.frames # but if you like tidyverse tibbles (the default with read_excel) # then just pass tibble = TRUE sheets \u0026lt;- readxl::excel_sheets(filename) x \u0026lt;- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X)) if(!tibble) x \u0026lt;- lapply(x, as.data.frame) names(x) \u0026lt;- sheets x }  #start to read and write data into csv file worksheets \u0026lt;- read_excel_allsheets(\u0026quot;data1.xlsx\u0026quot;) source(\u0026quot;cleaning.R\u0026quot;) filedata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]]))) write.table(filedata, \u0026quot;myDF.csv\u0026quot;, sep = \u0026quot;,\u0026quot;, col.names = !file.exists(\u0026quot;myDF.csv\u0026quot;), row.names=FALSE, append = T) worksheets \u0026lt;- read_excel_allsheets(\u0026quot;data2.xlsm\u0026quot;) source(\u0026quot;cleaning.R\u0026quot;) filedata \u0026lt;- do.call(rbind,lapply(names(worksheets), function(x) cleaner(worksheets[[x]]))) write.table(filedata, \u0026quot;myDF.csv\u0026quot;, sep = \u0026quot;,\u0026quot;, col.names = !file.exists(\u0026quot;myDF.csv\u0026quot;), row.names=FALSE, append = T) #end  The above code read 2 data files data1.xlsx and data2.xlsm, combining all worksheets of these two files and then write the data frame to a CSV file named myDF.csv. If we have more than 2 files to read and combine, we can just copy the code and paste it under and remember to change the source data file. We can also automate this process by making a loop, which will be presented in other post.\nOnce we have the combined CSV file, we can use NaviCAT to import this file to MySQL for further enhancement as presented in section 3. In the end, we can summary the production and energy consumption data in day or week that will be useful for visualization, correlation, and regression analysis.\nVisualization Tableu \n\nR library(DBI) library(RODBC) library(RMySQL) library(xts) library(ggplot2) library(hydroTSM) #call the hydrology package for time series analysis #this code is used for energy audit of the plant based on production and power consumption dataproduction = dbConnect(MySQL(), user='root', password='', dbname='exampledata', host='localhost') #exampledata is the database stored in your MySQL server dbListTables(dataproduction) dbListFields(dataproduction, 'analysisdate') rs = dbSendQuery(dataproduction, \u0026quot;select * from analysisdate\u0026quot;) rs=dbFetch(rs, n = -1) data=c(rs[1],rs[4],rs[5],rs[6]) a=as.Date(data$date,\u0026quot;%Y-%m-%d\u0026quot;) production=data$total_pro_hour power=data$total_power_hour ratio=data$ratio data=data.frame(a,production,power,ratio) #Correlation analysis hydropairs(data[,2:3]) #Plot production data plot.new() p=ggplot(data=data,aes(a, production))+ geom_line(color = \u0026quot;#00AFBB\u0026quot;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12)) p min \u0026lt;- as.Date(\u0026quot;2017-01-01\u0026quot;) max \u0026lt;- as.Date(\u0026quot;2018-10-30\u0026quot;) #p + scale_x_date(limits = c(min, max)) pp=p+scale_x_date(date_labels = \u0026quot;%Y\u0026quot;,date_breaks=\u0026quot;year\u0026quot;,limits = c(min, max))+labs(title = \u0026quot;Production hourly\u0026quot;,x=\u0026quot;Years\u0026quot;,y=\u0026quot;ML\u0026quot;)+scale_y_continuous(limits=c(0,2))#+ stat_smooth(method=\u0026quot;lm\u0026quot;) ggsave(\u0026quot;ch05_fig_energy_production.png\u0026quot;, plot = pp) print(pp) #Plot power consumption data plot.new() q=ggplot(data=data,aes(a, power))+ geom_line(color = \u0026quot;#00AFBB\u0026quot;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12)) #q min \u0026lt;- as.Date(\u0026quot;2017-01-01\u0026quot;) max \u0026lt;- as.Date(\u0026quot;2018-10-30\u0026quot;) #p + scale_x_date(limits = c(min, max)) qq=q+scale_x_date(date_labels = \u0026quot;%Y\u0026quot;,date_breaks=\u0026quot;year\u0026quot;,limits = c(min, max))+labs(title = \u0026quot;Power hourly\u0026quot;,x=\u0026quot;Years\u0026quot;,y=\u0026quot;KW\u0026quot;)+scale_y_continuous(limits=c(0,250))#+ stat_smooth(method=\u0026quot;lm\u0026quot;) ggsave(\u0026quot;ch05_fig_energy_power.png\u0026quot;, plot = qq) print(qq) #plot ratio plot.new() w=ggplot(data=data,aes(a, ratio))+ geom_line(color = \u0026quot;#00AFBB\u0026quot;) + theme(axis.text.x = element_text(angle = 60,hjust=1))+theme_gray(base_size = 14)+ theme(plot.title = element_text(size=12)) #w min \u0026lt;- as.Date(\u0026quot;2017-01-01\u0026quot;) max \u0026lt;- as.Date(\u0026quot;2018-10-30\u0026quot;) #p + scale_x_date(limits = c(min, max)) ww=w+scale_x_date(date_labels = \u0026quot;%Y\u0026quot;,date_breaks=\u0026quot;year\u0026quot;,limits = c(min, max))+labs(title = \u0026quot;Ratio\u0026quot;,x=\u0026quot;Years\u0026quot;,y=\u0026quot;\u0026quot;)+scale_y_continuous(limits=c(0,250))#+ stat_smooth(method=\u0026quot;lm\u0026quot;) ggsave(\u0026quot;ch05_fig_energy_ratio.png\u0026quot;, plot = ww) print(ww) dbDisconnect(dataproduction)  \n\n\n\n","date":1568561445,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1568561445,"objectID":"6fdc373de278252ecd468c27839f3f14","permalink":"/vi/post/2019-09-15-excel-combine/","publishdate":"2019-09-15T23:30:45+08:00","relpermalink":"/vi/post/2019-09-15-excel-combine/","section":"post","summary":"What if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats? not only that, within each Excel file, there are hundreds of worksheets with in-homogeneous structures –F.. Excel ^_^, but that is the reality, particularly when we have to work with Clients whose business are not much involved with TECH.\nWhat if our Client provides us with a number of poorly recorded databases with thousands of records in different Excel formats?","tags":["excel","mutiple worksheet"],"title":"Excel worksheets combination","type":"post"},{"authors":["Juergen Hackl and Bryan T Adey and **Nam Lethanh**"],"categories":[],"content":"","date":1531872000,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1531872000,"objectID":"6336823b86b03db2f61c09ccd745fb45","permalink":"/vi/publication/hackl2018/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/hackl2018/","section":"publication","summary":"Disruptive events, such as earthquakes, floods, and landslides, may disrupt the service provided by transportation networks on a vast scale, as their occurrence is likely to cause multiple objects to fail simultaneously. The restoration program following a disruptive event should restore service as much, and as fast, as possible. The estimation of risk due to natural hazards must take into consideration the resilience of the network, which requires estimating the restoration program as accurately as possible. In this article, a restoration model using simulated annealing is formulated to determine near‐optimal restoration programs following the occurrence of hazard events. The objective function of the model is to minimize the costs, taking into consideration the direct costs of executing the physical interventions, and the indirect costs that are being incurred due to the inadequate service being provided by the network. The constraints of the model are annual and total budget constraints, annual and total resource constraints, and the specification of the number and type of interventions to be executed within a given time period. The restoration model is demonstrated by using it to determine the near‐optimal restoration program for an example road network in Switzerland following the occurrence of an extreme flood event. The strengths and weaknesses of the restoration model are discussed, and an outlook for future work is given.","tags":["Weibull","Tunnel","Asset Management"],"title":"Determination of Near‐Optimal Restoration Programs for Transportation Networks Following Natural Hazard Events Using Simulated Annealing","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey and Marcel Burkhalter"],"categories":[],"content":"","date":1520640000,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1520640000,"objectID":"64e7afb2e98876016d77dea953c322e4","permalink":"/vi/publication/lethanh2018/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/lethanh2018/","section":"publication","summary":"A road network consists of multiple objects that deteriorate over time with different speeds of deterioration. In order to provide an adequate level of service over time, these objects will eventually require interventions. As road managers are trying, in general, to maximize the benefit obtained from the road network, it is in their interest to determine intervention programs, which consist of the grouping of interventions in work zones. The determination of optimal intervention programs is relatively complicated when considering single objects, but it becomes even more so when considering multiple objects embedded within a network. The objects to be included in the work zones at each time interval depend on many factors, such as the interventions to be executed on the objects, the maximum allowable length of the work zones, the traffic configurations to be used in the work zones and the available financial resources. Although some initial research in this area has been conducted, none has determined the optimal set of work zones on large infrastructure networks in a geographical information system (GIS) framework, which is necessary in the world of modern infrastructure management. In the work presented in this paper, a GIS-based program was developed to determine optimal intervention programs for large infrastructure networks using a linear optimization model, which can be linked directly to a GIS. The model includes constraints on the amount of available resources, on the length of the work zone, and on the distance between two work zones. A constraint-constructing algorithm is used in order to set up the latter two constraints. The program is illustrated by determining the optimal set of work zones for an example road network similar to the one in the canton of Wallis, Switzerland, including more than 2,000 bridges, tunnels, and road sections.","tags":["Optimization","GIS","Workzone","Asset Management"],"title":"Determining an Optimal Set of Work Zones on Large Infrastructure Networks in a GIS Framework","type":"publication"},{"authors":["Nam Lethanh  and Juergen Hackl  and Bryan T. Adey"],"categories":[],"content":"","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1500249600,"objectID":"66baddc42ade320793f9301b43f0a475","permalink":"/vi/publication/lethanh2017/","publishdate":"2017-07-17T00:00:00Z","relpermalink":"/vi/publication/lethanh2017/","section":"publication","summary":"Many bridge management systems use Markov models to predict the future deterioration of structural elements. This information is subsequently used in the determination of optimal intervention strategies and intervention programs. The input for these Markov models often consists of the condition states of the elements and how they have changed over time. This input is used to estimate the probabilities of transition of an object from each possible condition state to each other possible condition state in one time period. A complication in using Markov models is that there are situations in which there is an inadequate amount of data to estimate the transition probabilities using traditional methods (e.g., due to the lack of recording past information so that it can be easily retrieved, or because it has been collected in an inconsistent or biased manner). In this paper, a methodology to estimate the transition probabilities is presented that uses proportional data obtained by mechanistic-empirical models of the deterioration process. A restricted least-squares optimization model is used to estimate the transition probabilities. The methodology is demonstrated by using it to estimate the transition probabilities for a reinforced concrete (RC) bridge element exposed to chloride-induced corrosion. The proportional data are generated by modeling the corrosion process using mechanistic-empirical models and Monte Carlo simulations.","tags":["Markov","BMS","Asset Management","Regression","Optimization"],"title":"Determination of Markov Transition Probabilities to be Used in Bridge Management from Mechanistic-Empirical Models","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey"],"categories":[],"content":"","date":1482019200,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1482019200,"objectID":"18a162934edd1f4d524f8c6d1cf14f1f","permalink":"/vi/publication/lethanh2016g/","publishdate":"2016-12-18T00:00:00Z","relpermalink":"/vi/publication/lethanh2016g/","section":"publication","summary":"In this paper, a real option approach to determine the optimal time to execute interventions on rail infrastructure, when it is not known for certain which intervention is to be executed, is presented (i.e. the optimal intervention window). Such an approach is useful in the management of rail infrastructure that belongs to a multi-national rail corridor where multiple railway organizations, ideally, should coordinate their maintenance interventions, years in advance, to minimize service disruptions. The approach is based on an adaptation of the Black and Scholes differential equation model used to value European call options in financial engineering. It is demonstrated by determining the optimal intervention window for infrastructure in a fictive rail corridor.","tags":["Weibull","Railway","Real Option","Asset Management"],"title":"A real option approach to determine optimal intervention windows for multi-national rail corridor","type":"publication"},{"authors":["Nam Lethanh and Craig Richmond and Bryan T. Adey"],"categories":[],"content":"","date":1473465600,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1473465600,"objectID":"3eb4f586e356c9b76748d7d05c962fbb","permalink":"/vi/publication/lethanh2016b/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/lethanh2016b/","section":"publication","summary":"In the management of road networks, it is often desired to know the condition of individual road sections, which is approximated using the values of condition indicators. The values of these indicators can be used, for example, to determine whether an intervention should be executed on the road sections in the upcoming year, or to predict the future condition of the road sections. Unfortunately, a common problem when working with these data is that there are numerous road sections where no information is available. This can happen either due to errors made during the inspection campaigns themselves or due to using multiple independent sets of geographical information system (GIS) indexed data, when the sets are recorded as noncoincidental GIS shapes. It is of interest to the road manager to estimate the values of the missing condition indicators as best as possible. In this paper, an investigation of the ability to estimate values of road section indicators based on their spatial correlation is presented. The investigation is done by estimating the values of condition indicators for surface defects, and longitudinal and transversal unevenness exploiting the spatial correlation between them, on the Swiss national highway network. It is shown that the values of road section indicators can be estimated based on their spatial correlation with reasonably high levels of accuracy. The variation of the predictive ability per condition indicator is shown.","tags":["PMS","Krigging","Asset Management"],"title":"Investigation of the Ability to Estimate Values of Road Section Condition Indicators Based on Their Spatial Correlation","type":"publication"},{"authors":["Nam Lethanh and Jurgen Hackl and Bryan T. Adey"],"categories":[],"content":"","date":1466899200,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1466899200,"objectID":"b123b518865a3393cdcbb86fed20f133","permalink":"/vi/publication/lethanh2016f/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/lethanh2016f/","section":"publication","summary":"","tags":["Regression Analysis","Markov","Asset Management","Crack","Corrosion","BMS"],"title":"Estimating Markov Transition Probabilities for Reinforced Concrete Bridges based on Mechanistic-Empirical Corrosion Models","type":"publication"},{"authors":[],"categories":["Project Management","Feasibility Study"],"content":" The Project Objectives Responsibilities Lesson Learned Authorship ","date":1461035302,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1461035302,"objectID":"89f7c19e0a56791915b34d18decd0876","permalink":"/vi/project/2016-kepco/","publishdate":"2016-04-19T11:08:22+08:00","relpermalink":"/vi/project/2016-kepco/","section":"project","summary":"Feasibility Study for Pangasinan 900MW CFBC Power Station","tags":["Project Management","Feasibility Study","Financial Model","Power Stations"],"title":"900MW Power Station - KEPCO E\u0026C","type":"project"},{"authors":["Nam Lethanh and Kiyoshi Kobayashi and Kiyoyuki Kaito"],"categories":[],"content":"","date":1411104032,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1411104032,"objectID":"2b6cc9ac4a65d38735ba671728675e80","permalink":"/vi/publication/lethanh2014d/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/lethanh2014d/","section":"publication","summary":"The deterioration of a pavement surface can be described in terms of the presence and severity of distinct distresses, like potholes, cracking, and rutting. Each deterioration process is ordinarily described by a set of pavement indicators (e.g., number of potholes, percentage of cracks, international roughness index) that are measured during monitoring and inspection activities. Manifestly, there exist statistical correlations among the deterioration processes. For instance, cracks appearing on a road section may contribute to an increase in pothole occurrence, and vice versa. In order to mathematically formulate the statistical interdependency among deterioration processes, a Poisson hidden Markov model is proposed in this paper. The model describes the complex process of pavement deterioration, which includes the frequent occurrence of local damage (e.g., potholes) as well as the degradation of other pavement indicators (e.g., cracks, roughness). To model the concurrent frequency of local damage, a stochastic Poisson process is used. At the same time, a Markov chain model is used to depict the degradation of other pavement indicators. A numerical estimation approach using Bayesian statistics with a Markov chain Monte Carlo simulation is developed to derive the values of the model’s parameters based on historical information. The applicability of the model was demonstrated through an empirical example using data from a Japanese highway road link..","tags":["Markov","Bayesian","MCMC","Poisson Process","PMS","Asset Management"],"title":"Infrastructure Deterioration Prediction with a Poisson Hidden Markov Model on Time Series Data","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey and Dilum N. Fernando"],"categories":[],"content":"","date":1395360000,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1395360000,"objectID":"6da4c28ecefac3a67567fe167573f72b","permalink":"/vi/publication/lethanh2014/","publishdate":"2014-03-21T00:00:00Z","relpermalink":"/vi/publication/lethanh2014/","section":"publication","summary":"In the existing infrastructure management systems, optimal interventions strategies (OISs) are determined for objects that deteriorate gradually (manifest deterioration process, MDPs), under the assumption that with appropriate inspection and intervention strategies the probability of failure of object can be neglected. Objects that deteriorate suddenly (latent deterioration process, LDPs), for example, due to scouring during a flood or earth movements during an earthquake are not considered. The determination of OISs for an object that deteriorates due to both MDPs and LDPs requires the consideration of both. The latter, however, means that the probability of failure of the object must be considered. In this article, a Markov model is presented that can be used to determine OISs for multiple objects of multiple types affected by uncorrelated MDPs and LDPs. The model is an extension of the model proposed by Mayet and Madanat (Incorporation of seismic considerations in bridge management systems. Computer-Aided Civil and Infrastructure Engineering, 17:185–193, 2002). In the model, a set of condition states (CSs) is used to describe the condition of objects of each type, where each set is composed of non-failure CSs and failure CSs. The probabilities of going from each non-failure CS to each failure CS are estimated using normalised fragility curves, and the probabilities of going from each non-failure CS to each non-failure CS are initially estimated using the Markov deterioration prediction model of Kobayashi, Kaito, and Lethanh (A Bayesian estimation method to improve deterioration prediction for infrastructure system with Markov chain model. International Journal of Architecture, Engineering and Construction, 1:1–13, 2012a) and later adjusted taking into consideration the probabilities of entering the failure CSs. The use of the model is demonstrated using a road link comprising one road section and one bridge.","tags":["Markov","Asset Management"],"title":"Optimal intervention strategies for multiple objects affected by manifest and latent deterioration processes","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey"],"categories":[],"content":"","date":1358812800,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1358812800,"objectID":"07bcc46ca117c67a35f841c7d66f2378","permalink":"/vi/publication/lethanh2013a/","publishdate":"2013-01-22T00:00:00Z","relpermalink":"/vi/publication/lethanh2013a/","section":"publication","summary":"In this paper, a probabilistic model for the determination of optimal intervention strategies (OISs) for a road link composed of multiple objects that are affected by gradual deterioration processes is investigated. The model is composed of a deterioration part and a strategy evaluation part. In the deterioration part, a Weibull hazard function is used to represent the deterioration of the individual objects, where the values of the model parameters are to be estimated using inspection data. A threshold condition state (CS) for each object is defined, at which an intervention must be executed. The results of the deterioration part are used as inputs in the strategy evaluation part, in which OISs for individual objects and for the link as a whole are determined. The determination of the optimal strategies takes into consideration impacts on multiple stakeholders. The model is demonstrated by determining the OISs for a fictive road link composed of one bridge and two road sections. The main strengths of the methodology are that past deterioration is taken into consideration and that it is possible to consider the execution of interventions simultaneously and, therefore, associated reductions in impacts that normally occur when interventions are grouped. The main weakness of the methodology is that the condition of the objects is represented using only two CSs, i.e. fully operational and not fully operational.","tags":["Weibull","BMS","PMS","Asset Management"],"title":"Investigation of the Use of a Weibull Model for the Deterioration of Optimal Road Link Intervention Strategies","type":"publication"},{"authors":["Nam Lethanh and Bryan T. Adey"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1356998400,"objectID":"4e531a8e3ae79617303ed317eed89ed1","permalink":"/vi/publication/lethanh2013b/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/vi/publication/lethanh2013b/","section":"publication","summary":"In this paper, the potential of using an exponential hidden Markov model to model an indicator of pavement condition as a hidden pavement deterioration process, i.e. one that is not directly measurable, is investigated. It is assumed that the evolution of the values of the pavement condition indices can be adequately described using discrete condition states and modelled as a Markov process. It is also assumed that the values of the indices can be measured over time and represented continuously using exponential distributions. The potential advantage of using such a model is illustrated using a real-world example.","tags":["PMS","Markov","Asset Management"],"title":"Use of Exponential Hidden Markov Models for Modeling Pavement Deterioration","type":"publication"},{"authors":["Kiyoshi Kobayashi and Kiyoyuki Kaito and **Nam Lethanh**"],"categories":[],"content":"","date":1337404832,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1337404832,"objectID":"5f497913beb4ff370360754d7306a3d9","permalink":"/vi/publication/kobayashi2012a/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/kobayashi2012a/","section":"publication","summary":"The application of Markov models as deterioration-forecasting tools has been widely documented in the practice of infrastructure management. The Markov chain models employ monitoring data from visual inspection activities over a period of time in order to predict the deterioration progress of infrastructure systems. Monitoring data play a vital part in the managerial framework of infrastructure management. As a matter of course, the accuracy of deterioration prediction and life cycle cost analysis largely depends on the soundness of monitoring data. However, in reality, monitoring data often contain measurement errors and selection biases, which tend to weaken the correctness of estimation results. In this paper, the authors present a hidden Markov model to tackle selection biases in monitoring data. Selection biases are assumed as random variables. Bayesian estimation and Markov Chain Monte Carlo simulation are employed as techniques in tackling the posterior probability distribution, the random generation of condition states, and the model’s parameters. An empirical application to the Japanese national road system is presented to demonstrate the applicability of the model. Estimation results highlight the fact that the properties of the Markov transition matrix have greatly improved in comparison with the properties obtained from applying the conventional multi-stage exponential Markov model.","tags":["Markov","Bayesian","MCMC","Gibbs Sampling","BMS","PMS","Asset Management"],"title":"A statistical deterioration forecasting method using hidden Markov model for infrastructure management","type":"publication"},{"authors":["Kiyoshi Kobayashi and Kiyoyuki Kaito and **Nam Lethanh**"],"categories":[],"content":"","date":1332134432,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1332134432,"objectID":"c216cb8eb5c36b369c59b327718ce4d3","permalink":"/vi/publication/kobayashi2012/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/kobayashi2012/","section":"publication","summary":"In many practices of bridge asset management, life cycle costs are estimated by statistical de-terioration prediction models based upon monitoring data collected through inspection activities.  In many applications, it is, however, often the case that the validity of statistical deterioration prediction models is flawed by an inadequate stock of inspection dates. In this paper, a systematic methodology is presented to provide estimates of the deterioration process for bridge managers based upon empirical judgments at early stages by experts, and whereby revisions may be made as new data are obtained through later inspections. More concretely, Bayesian estimation methodology is developed to improve the estimation of Markov transition probability of the multi-stage exponential Markov model by Markov chain Monte Carlo method using Gibbs sampling. The paper concludes with an empirical example,  using the real world monitoring data, to demonstrate the applicability of the model and its Bayesian estimation method in the case of incomplete monitoring data.","tags":["Markov","BMS","Bayesian","MCMC","Gibbs Sampling","Asset Management"],"title":"A Bayesian Estimation Method to Improve Deterioration Prediction for Infrastructure System with Markov Chain Model","type":"publication"},{"authors":["Kiyoshi Kobayashi and Kiyoyuki Kaito and **Nam Lethanh**"],"categories":[],"content":"","date":1273449600,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1273449600,"objectID":"1b1ab327263a7a50eb33e34da5b4d3d5","permalink":"/vi/publication/kobayashi2010a/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/kobayashi2010a/","section":"publication","summary":"In this paper, a time-dependent deterioration forecasting model is presented. In the model the deterioration process is described by transition probabilities, which are conditional upon actual in-service duration. The model is formulated by the multistage Weibull hazard model defined by using multiple Weibull hazard functions. The model can be estimated based upon inspection data that are obtained at discrete points in time. The applicability of the model and the estimation methodology presented in this paper are investigated against an empirical data set of highway utilities in the real world.","tags":["Weibull","Tunnel","Asset Management"],"title":"Deterioration Forecasting Model with Multistage Weibull Hazard Functions","type":"publication"},{"authors":["Nam Lethanh and Thao Nguyendinh and Kiyoyukia Kaito and Kiyoshi Kobayashi"],"categories":[],"content":"","date":1252540800,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1252540800,"objectID":"6b46a07f008aad49108a04954b0995a3","permalink":"/vi/publication/lethanh2009/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/lethanh2009/","section":"publication","summary":"","tags":["Markov","Asset Management","Benchmarking","PMS"],"title":"A Benchmarking Approach to Pavement Management: Lessons from Vietnam","type":"publication"},{"authors":["Nam Lethanh"],"categories":[],"content":"","date":1252540800,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1252540800,"objectID":"06b331e10472d650c571bf0aa06e53c4","permalink":"/vi/publication/lethanh2009c/","publishdate":"2019-10-19T13:20:32+08:00","relpermalink":"/vi/publication/lethanh2009c/","section":"publication","summary":"","tags":["Optimization","Markov","Weibull","PMS","BMS","Tunnel","Water Ultilities","Asset Management"],"title":"Stochastic Optimization Methods for Infrastructure Management with Incomplete Monitoring Data","type":"publication"},{"authors":["Nam Lethanh and Kengo Obama and Koyoshi Kobayashi"],"categories":[],"content":"","date":1213056000,"expirydate":-62135596800,"kind":"page","lang":"vi","lastmod":1213056000,"objectID":"a035b0c76f184310033b05d7b6b3aba3","permalink":"/vi/publication/lethanh2008c/","publishdate":"2008-06-10T00:00:00Z","relpermalink":"/vi/publication/lethanh2008c/","section":"publication","summary":"One of the core techniques for the Pavement Management Systems (PMS) is the deterioration hazard model. The hazard models stochastically forecast the deterioration progresses of pavement based on its historical performance indicators over time. This paper introduces a class of hazard models, which consider the heterogeneity factors by a local mixing mechanism. Special attention is paid to the formulation of transition probability of condition state, which is subjected to follow Markov chain. The paper further introduces optimization approach for selecting best repair strategy based on discounted life cycle cost analysis. The application of the model targets to minimize exposing risks to road administrators. The usefulness and practicability of the model are examined through life cycle costs evaluation with real data of Vietnamese highway system.","tags":["Markov","Asset Management"],"title":"Local mixtures hazard model: A semi-parametric approach to risk management in pavement system","type":"publication"}]